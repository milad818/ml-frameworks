{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3799cdc9-ee1e-44d9-ae02-f305de3f5ff3",
   "metadata": {},
   "source": [
    "## LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c737c4-b7e7-4279-8e39-f9913b5fa599",
   "metadata": {},
   "source": [
    "### What is LangChain?\n",
    "LangChain is an open-source framework designed to facilitate the development of applications that leverage large language models (LLMs) like OpenAI’s GPT, Hugging Face models, or similar technologies. Its primary goal is to make it easier to build applications that use LLMs in more structured, robust, and context-aware ways. LangChain focuses on combining LLMs with external data, chaining multiple components together for more complex workflows, and managing contextual information effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822394d9-7eaa-4949-aa68-0fff79e5cf89",
   "metadata": {},
   "source": [
    "To find out more about the repository devoted to open-source LangChain belongings and libraries click [here](https://github.com/langchain-ai/langchain)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20bf38ec-65ee-479f-b1db-e0af63d2b3cc",
   "metadata": {},
   "source": [
    "Below we will have a quick review of how to create a client, make a request and collect the corresponding response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d42d175-9896-4514-9096-3f8dfab2791b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import langchain\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5bf4b7c-eb92-4d3e-8cbb-435ac7132dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate API Key\n",
    "my_openaiapi_key = \"Add your own key here!\"\n",
    "\n",
    "# openai.api_key = my_openaiapi_key\n",
    "client = OpenAI(openai_api_key=my_openaiapi_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c1fb1b8-0a9c-4ba5-9b65-df8a1bff2448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Come up with a prompt\n",
    "prompt = \"Can you tell how many automobile companies there are in the world?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "96970e11-b43c-479a-af10-37373f65fb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.predict(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3f844a1c-fc37-45e2-955f-f9bebf252586",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('\\n\\nAs of 2021, there are approximately 98 automobile companies in the world.',\n",
       " 'As of 2021, there are approximately 98 automobile companies in the world.')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response, response.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0690b553-990c-427a-9b0c-44eef079ff4c",
   "metadata": {},
   "source": [
    "`strip()` removes all whitespace at the start and end, e.g., spaces, tabs, newlines and carriage returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "80683fd9-d2c8-4271-a133-a861237a03fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "As of 2021, there are approximately 98 automobile companies in the world.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c68f84-a0af-4eff-bd35-15d7b195ab0e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### List of Content\n",
    "- How to use OpenAI via LangChain\n",
    "- Prompt Templating\n",
    "- Agent\n",
    "- Chains\n",
    "- Document Loader\n",
    "- Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059ed66c-c293-48c7-b430-a18aef640b28",
   "metadata": {},
   "source": [
    "### How can we utilize OpenAI through LangChain?\n",
    "#### Why OpenAI API? Why not OpenAI API?\n",
    "Why OpenAI API?\n",
    "- **State-of-the-Art Models:** Provides access to GPT (e.g., GPT-4), which is among the most advanced language models available.\n",
    "- **Ease of Use:** Simple and intuitive API with robust documentation for seamless integration.\n",
    "- **Versatility:** Supports a wide range of applications, such as text generation, summarization, code completion, translation, and more.\n",
    "- **Scalability:** Handles workloads of various sizes, suitable for startups, enterprises, or research projects.\n",
    "- **No Need for Infrastructure:** Eliminates the need to manage hardware or train your own models.\n",
    "\n",
    "Why Not OpenAI API?\n",
    "- **Cost:** Pricing may be prohibitive for large-scale usage or for those with tight budgets.\n",
    "- **Data Privacy Concerns:** User data is sent to OpenAI’s servers, which may not align with strict data privacy requirements in sensitive industries.\n",
    "- **Dependence on External Service:** Reliance on an external API could lead to downtime or latency issues.\n",
    "- **Customization Limitations:** While powerful, the models are pre-trained and may not be fine-tuned for niche use cases without additional effort.\n",
    "- **Alternative Models:** Other open-source models like Hugging Face or LLaMA offer free, customizable, and local deployment options, which can be more suitable for certain projects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b431c7-782c-428e-aebd-5848cb77f2f2",
   "metadata": {},
   "source": [
    "#### What makes us use LangChain?\n",
    "LangChain is used because it simplifies building end-to-end applications that combine LLMs (Large Language Models) with other tools, enabling more dynamic and intelligent workflows.\n",
    "\n",
    "LangChain is used for simplifying the development of applications that integrate language models with external data sources, APIs, and workflows. It allows developers to chain together various components like language models, databases, and web scraping tools, enabling more complex interactions and tasks like document querying, summarization, and multi-step reasoning. LangChain also supports a modular approach for integrating multiple sources of information, offering features like prompt management, model wrappers, and memory management, which make it ideal for building scalable, customizable, and efficient applications in areas like chatbots, document analysis, and knowledge extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816ae752-6138-4b27-b9f0-b2a2f64a3ae5",
   "metadata": {},
   "source": [
    "### Prompt Template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368e0a64-0225-4461-8aba-83f6cbfbca35",
   "metadata": {},
   "source": [
    "In order to dig deeper into the world of LangChain, let's firstly discuss what **Prompt Templates** (`langchain.prompts.prompt.PromptTemplate()`) are. A prompt template is a scaffold or blueprint that guides the AI in generating a response. It helps to translate user input and parameters into instructions for a language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbbaf75f-ec47-4b86-861e-cb47976d17f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variable=[\"product\"],\n",
    "    template=\"Can you tell how many {product} companies there are in the world?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f680f1b4-69d0-4b69-a9ed-5c1537d1bdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template2 = PromptTemplate(\n",
    "    input_variable=[\"product\"],\n",
    "    template=\"Can you tell approximately how many {product} companies there are in the Italy?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "561b42f9-da35-49ef-b10b-8d0e500467af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a few prompts based on the template\n",
    "prompt1 = prompt_template.format(product=\"mobile phone\")\n",
    "prompt2 = prompt_template2.format(product=\"start-up\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "85d30cf2-9c2b-4e6c-b8c8-1141544219e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It is difficult to determine the exact number of mobile phone companies in the world as new companies are constantly emerging and existing companies may merge or go out of business. However, as of 2021, there are over 100 major mobile phone companies operating globally. Some of the largest companies include Samsung, Apple, Huawei, Xiaomi, and Oppo.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.predict(prompt1).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f04e542-0390-47e4-8ff0-c1dbca90f89c",
   "metadata": {},
   "source": [
    "**REMEMBER** that you can also pass the argument `max_tokens` to limit the output tokens!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2eb266a1-1213-4906-8630-1b5afc4885c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It is difficult to provide an exact number as the number of start-up companies in Italy is constantly changing. However, according to a report by Startup Genome, there were approximately 10,000 active start-ups in Italy in 2019. This'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.predict(prompt2, max_tokens=50).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904500a9-c697-4620-99d8-c6874e4d234f",
   "metadata": {},
   "source": [
    "#### What is strange about the response produced above?\n",
    "Oh, yes! You're right. The response is incomplete according to the limitation we have applied to the number of output tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0610fb0-9d43-4b2c-ac8f-b5a2e7662eb1",
   "metadata": {},
   "source": [
    "An alternative way to create a prompt template is `PromptTemplate.from_template()` not passing the input variable explicitly. Let's see how it works below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c965873c-4a9b-4002-8367-6a574c43b710",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template3 = PromptTemplate.from_template(\"What is a synonym to the word {word} that you can come up with?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d21618bb-e329-4e5f-817d-75e306e20b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt3=prompt_template3.format(word=\"habit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f5e554a1-c601-4ee2-a718-db3e1767bdac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Routine'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.predict(prompt3).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c616d9c4-ba07-4c23-89fc-affa3c2f4456",
   "metadata": {},
   "source": [
    "### Agents\n",
    "An agent in an LLM application is a mechanism that enables the language model to **interact with external tools, APIs, or environments** dynamically. It interprets user inputs, decides which actions to take (e.g., calling APIs, retrieving documents, or running computations), and integrates the results into coherent responses. This allows for complex, task-oriented workflows, such as executing code, answering questions from databases, or performing **real-world** and in some cases **real-time** tasks, making the application more interactive and functional.\n",
    "\n",
    "**SerpAPI** is an API that provides search engine results. An agent, in this context, would be the system or mechanism (often part of an LLM application) that decides to query the SerpAPI, processes the returned results, and incorporates them into its output. For a Google search engine, the agent would typically be the logic or program (like LangChain's tools or OpenAI's function calling) that triggers a search using SerpAPI and handles the results effectively. SerpAPI does not retrieve information solely through Google. While it is well-known for integrating with Google Search, SerpAPI supports multiple search engines and platforms, including Bing, DuckDuckGo, Yahoo, and others, as well as specialized services like Google Maps, Google Images, and more. This versatility allows it to retrieve information from various sources, not just Google.\n",
    "\n",
    "`google-search-engine` is a package meant to scrape and parse search results from Google, Bing, Baidu, Yandex, Yahoo, Home Depot, eBay and more, using SerpApi. The following services are provided: Search API. Search Archive API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c9091cd-b6e1-4f72-9983-1f4973a9e17a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-search-results in c:\\users\\asus k513eq\\miniconda3\\lib\\site-packages (2.4.2)\n",
      "Requirement already satisfied: requests in c:\\users\\asus k513eq\\miniconda3\\lib\\site-packages (from google-search-results) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\asus k513eq\\miniconda3\\lib\\site-packages (from requests->google-search-results) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asus k513eq\\miniconda3\\lib\\site-packages (from requests->google-search-results) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\asus k513eq\\miniconda3\\lib\\site-packages (from requests->google-search-results) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus k513eq\\miniconda3\\lib\\site-packages (from requests->google-search-results) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install google-search-results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8c4d499-e75a-40d2-a8ac-9ba94f22e0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate SerpAPI key\n",
    "my_serpapi_key = \"Add your own key here!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e77bd733-07cf-43b9-80cb-ef1772c078e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.agents import initialize_agent, AgentType, load_tools\n",
    "\n",
    "# Load tools - they allow agents to interact with various resources and services like APIs, databases, file systems, etc.\n",
    "tools = load_tools([\"serpapi\"], serpapi_api_key=my_serpapi_key, llm=client)\n",
    "\n",
    "# Load an agent executor given tools and LLM\n",
    "agent = initialize_agent(tools, client, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94d92e0a-bab1-4cac-8316-630d110f6b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS K513EQ\\AppData\\Local\\Temp\\ipykernel_16424\\3405584166.py:1: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  agent.run(\"Who won Open Australia in 2021?\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m You should always think about what to do\n",
      "Action: Search\n",
      "Action Input: \"Open Australia 2021 winner\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m{'title': 'Australian Open', 'date': 'Jan 10–Feb 22, 2021', 'tables': {'title': \"Women's Singles\", 'games': [{'date': 'Sat Feb 20', 'stage': 'Final', 'location': 'Rod Laver Arena', 'status': 'Final', 'video_highlights': {'link': 'https://ausopen.com/match/2021-jennifer-brady-vs-naomi-osaka-ws701#!media?g=video-6233932784001', 'thumbnail': 'https://serpapi.com/searches/6799187ed959597baef45f34/images/7306be2960b5a85313a35411f99a753b03c2f8dfba89a0eb2d80884dd8c53e9755b2e90be0fc83fc930720e710985f3183c4eae1af9c89fca6fbbea828fa4dd9.jpeg', 'duration': '2:00'}, 'players': [{'name': 'J. Brady', 'ranking': '22', 'thumbnail': 'https://serpapi.com/searches/6799187ed959597baef45f34/images/7306be2960b5a85313a35411f99a753b03c2f8dfba89a0eb7f7addea4be0791551988c858d272ac0976428454f9e5288f74410045e50db24.png', 'sets': {'set-1': '4', 'set-2': '3'}}, {'name': 'N. Osaka', 'ranking': '3', 'thumbnail': 'https://serpapi.com/searches/6799187ed959597baef45f34/images/7306be2960b5a85313a35411f99a753b03c2f8dfba89a0eb7f7addea4be07915c3e897bf5e36a97a6211d9872384354e9ae4f2b03ac3a94a.png', 'sets': {'set-1': '6', 'set-2': '6'}}]}]}}\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: Naomi Osaka\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Naomi Osaka'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"Who won Open Australia in 2021?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5f0eb5-0e85-401e-8d70-7347c2366b60",
   "metadata": {},
   "source": [
    "Now let's try a different resource, for instance, ArXiv API (also Wikipedia), for accessing scientific papers and metadata from the ArXiv repository. Firstly, you have to install `arxiv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3be9b359-7ca2-4b86-9abc-1af885b11b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install arxiv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464955ed-fba4-4e19-87a6-c2ab6c285330",
   "metadata": {},
   "source": [
    "**Note** that one can pass several tools or resources to `load_tools()` as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "18461445-fba8-4a2a-9bab-d4d4eaea356b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = load_tools([\"arxiv\", \"serpapi\"], serpapi_api_key=my_serpapi_key, llm=client)\n",
    "\n",
    "agent = initialize_agent(tools, client, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "280c548b-edf9-438e-8bdb-12294c9215d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS K513EQ\\AppData\\Local\\Temp\\ipykernel_18140\\2575220378.py:1: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  agent.run(\"Could you please provide me with the most recent papers on breast cancer exploiting vision transformers since 2024?\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I should search on arxiv for papers related to breast cancer and vision transformers since 2024.\n",
      "Action: arxiv\n",
      "Action Input: \"breast cancer vision transformers since 2024\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mPublished: 2024-06-29\n",
      "Title: Development of an interactive GUI using MATLAB for the detection of type and stage of Breast Tumor\n",
      "Authors: Poulmi Banerjee, Satadal Saha\n",
      "Summary: Breast cancer is described as one of the most common types of cancer which\n",
      "has been diagnosed mainly in women. When compared in the ratio of male to\n",
      "female, it has been duly found that the prone of having breast cancer is more\n",
      "in females than males. Breast lumps are classified mainly into two groups\n",
      "namely: cancerous and non-cancerous. When we say that the lump in the breast is\n",
      "cancerous, it means that it can spread via lobules, ducts, areola, stroma to\n",
      "various organs of the body. On the other hand, non-cancerous breast lumps are\n",
      "less harmful but it should be monitored under proper diagnosis to avoid it\n",
      "being transformed to cancerous lump. To diagnose these breast lumps the method\n",
      "of mammogram, ultrasonic images and MRI images are undertaken. Also, for better\n",
      "diagnosis sometimes doctors recommend for biopsy and any unforeseen anomalies\n",
      "occurring there may give rise to inaccurate test report. To avoid these\n",
      "discrepancies, processing the mammogram images is considered to be one of the\n",
      "most reliable methods. In the proposed method MATLAB GUI is developed and some\n",
      "sample images of breast lumps are placed accordingly in the respective axes.\n",
      "With the help of sliders the actual breast lump image is compared with the\n",
      "already stored breast lump sample images and then accordingly the history of\n",
      "the breast lumps is generated in real time in the form of test report.\n",
      "\n",
      "Published: 2024-09-05\n",
      "Title: Deep Transfer Learning for Breast Cancer Classification\n",
      "Authors: Prudence Djagba, J. K. Buwa Mbouobda\n",
      "Summary: Breast cancer is a major global health issue that affects millions of women\n",
      "worldwide. Classification of breast cancer as early and accurately as possible\n",
      "is crucial for effective treatment and enhanced patient outcomes. Deep transfer\n",
      "learning has emerged as a promising technique for improving breast cancer\n",
      "classification by utilizing pre-trained models and transferring knowledge\n",
      "across related tasks. In this study, we examine the use of a VGG, Vision\n",
      "Transformers (ViT) and Resnet to classify images for Invasive Ductal Carcinoma\n",
      "(IDC) cancer and make a comparative analysis of the algorithms. The result\n",
      "shows a great advantage of Resnet-34 with an accuracy of $90.40\\%$ in\n",
      "classifying cancer images. However, the pretrained VGG-16 demonstrates a higher\n",
      "F1-score because there is less parameters to update. We believe that the field\n",
      "of breast cancer diagnosis stands to benefit greatly from the use of deep\n",
      "transfer learning. Transfer learning may assist to increase the accuracy and\n",
      "accessibility of breast cancer screening by allowing deep learning models to be\n",
      "trained with little data.\n",
      "\n",
      "Published: 2024-01-20\n",
      "Title: Deep Learning in Breast Cancer Imaging: A Decade of Progress and Future Directions\n",
      "Authors: Luyang Luo, Xi Wang, Yi Lin, Xiaoqi Ma, Andong Tan, Ronald Chan, Varut Vardhanabhuti, Winnie CW Chu, Kwang-Ting Cheng, Hao Chen\n",
      "Summary: Breast cancer has reached the highest incidence rate worldwide among all\n",
      "malignancies since 2020. Breast imaging plays a significant role in early\n",
      "diagnosis and intervention to improve the outcome of breast cancer patients. In\n",
      "the past decade, deep learning has shown remarkable progress in breast cancer\n",
      "imaging analysis, holding great promise in interpreting the rich information\n",
      "and complex context of breast imaging modalities. Considering the rapid\n",
      "improvement in deep learning technology and the increasing severity of breast\n",
      "cancer, it is critical to summarize past progress and identify future\n",
      "challenges to be addressed. This paper provides an extensive review of deep\n",
      "learning-based breast cancer imaging research, covering studies on mammogram,\n",
      "ultrasound, magnetic resonance imaging, and digital pathology images over the\n",
      "past decade. The major deep learning methods and applications on imaging-based\n",
      "screening, diagnosis, treatment respon\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m After searching on arxiv, I found three relevant papers on breast cancer and vision transformers since 2024.\n",
      "Thought: I now know the final answer.\n",
      "Final Answer: The most recent papers on breast cancer exploiting vision transformers since 2024 are \"Development of an interactive GUI using MATLAB for the detection of type and stage of Breast Tumor,\" \"Deep Transfer Learning for Breast Cancer Classification,\" and \"Deep Learning in Breast Cancer Imaging: A Decade of Progress and Future Directions.\"\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The most recent papers on breast cancer exploiting vision transformers since 2024 are \"Development of an interactive GUI using MATLAB for the detection of type and stage of Breast Tumor,\" \"Deep Transfer Learning for Breast Cancer Classification,\" and \"Deep Learning in Breast Cancer Imaging: A Decade of Progress and Future Directions.\"'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"Could you please provide me with the most recent papers on breast cancer exploiting vision transformers since 2024?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9159dcc4-f4e0-4e29-9452-4e671e4b9ec0",
   "metadata": {},
   "source": [
    "### Chains\n",
    "According to LangChain official documentation, chains refer to sequences of calls - whether to an LLM, a tool, or a data preprocessing step. The primary supported way to do this is with [LCEL](\"https://python.langchain.com/v0.1/docs/expression_language/\") (LangChain Expression Language)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "09b222bd-15ea-4d76-9c76-c3c1c2a59457",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "prompt_template4 = PromptTemplate.from_template(\"In a word, which country is famous for its {product}?\")\n",
    "# prompt4=prompt_template4.format(product=\"chocolate\") # This step is not needed, automatically taken care of!\n",
    "# type(prompt_template4), type(prompt4)\n",
    "# print(prompt_template4)\n",
    "# print(prompt4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e5e708b5-dc43-46e9-90c6-226e37193a98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Switzerland.'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run query against any LLM\n",
    "chain = LLMChain(llm=client, prompt=prompt_template4)\n",
    "chain.run(\"chocolate\").strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0869b945-d3aa-4e09-9f64-728cc19158d6",
   "metadata": {},
   "source": [
    "#### Chains in LangChain are not necessarily about chaining multiple LLMs but about combining LLMs with other tools and processes. For example:\n",
    "\n",
    "- **Simple Chain:** Taking user input, formatting it into a prompt, querying an LLM, and returning the response.\n",
    "- **Complex Chain:** Integrating an LLM to answer a query, using its output to fetch data from an external API or database, and summarizing the results using the LLM again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d4fe9abd-14b6-4c5e-8c5b-d20bc70c8f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Chain\n",
    "\n",
    "prompt_template_meal = PromptTemplate(\n",
    "    input_variables=[\"meal\"],\n",
    "    template=\"What dish do you suggest I should have for {meal} today?\"\n",
    ")\n",
    "\n",
    "\n",
    "meal_request = LLMChain(llm=client, prompt=prompt_template_meal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "319821ea-513f-4908-b607-aba8e5ff9851",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_recepie = PromptTemplate(\n",
    "    input_variables=[\"dish\"],\n",
    "    template=\"Provide me with the ingredients for {dish}.\"\n",
    ")\n",
    "\n",
    "recepie_request = LLMChain(llm=client, prompt=prompt_template_recepie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0647e9d8-42ec-416b-8471-5d606eeb7cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SimpleSequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6a9c5e9a-99eb-4b08-9d5f-3044a817865d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Form a chain associating the two requests\n",
    "simple_chain = SimpleSequentialChain(chains=[meal_request, recepie_request])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2babfe4-1136-4d3a-a2b6-3b5621197f8d",
   "metadata": {},
   "source": [
    "Below the command `simple_chain.run(\"lunch\")` runs a predefined sequence of steps (the chain) using \"hunger\" as the input, and returns the final output after completing all the steps in the chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "dec7deab-1e47-499c-99d1-2c2989ee5845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain.invoke(dish_request)\n",
    "suggested_dish = simple_chain.run(\"lunch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1a5d4f46-625e-4ea6-b6b9-381e9f02345f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Here are some suggested ingredients:\n",
      "\n",
      "For the grilled chicken salad:\n",
      "- Boneless, skinless chicken breasts\n",
      "- Mixed greens (such as spinach, arugula, or spring mix)\n",
      "- Cherry tomatoes\n",
      "- Cucumber\n",
      "- Bell pepper\n",
      "- Red onion\n",
      "- Olive oil\n",
      "- Balsamic vinegar\n",
      "- Salt and pepper\n",
      "\n",
      "For the vegetarian stir-fry:\n",
      "- Assorted vegetables (such as broccoli, carrots, mushrooms, bell peppers, onions, and snow peas)\n",
      "- Tofu or tempeh (optional)\n",
      "- Soy sauce\n",
      "- Garlic\n",
      "- Ginger\n",
      "- Brown sugar\n",
      "- Rice or noodles\n",
      "\n",
      "For the classic sandwich:\n",
      "- Bread (such as whole wheat, sourdough, or ciabatta)\n",
      "- Deli meat (such as turkey, ham, or roast beef)\n",
      "- Cheese (such as cheddar, Swiss, or provolone)\n",
      "- Lettuce\n",
      "- Tomato\n",
      "- Avocado\n",
      "- Bacon\n",
      "- Mayo or mustard (optional)\n",
      "- Chips or soup (optional side dish)\n"
     ]
    }
   ],
   "source": [
    "print(suggested_dish)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9a0d11-2a7f-43d8-b9a6-49959ecb7b07",
   "metadata": {},
   "source": [
    "#### Can we also retrieve the intermediate answers?\n",
    "There are ways to access the intermediate responses of a chain. One could be through setting `output_key` and `output_variables` in the request and chain respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "2a3c4e37-b7ee-445e-8d59-3ba262c17750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define a prompt for step 1\n",
    "step_1_prompt = PromptTemplate(\n",
    "    input_variables=[\"input_text\"],\n",
    "    template=\"Extract a name from the following text: {input_text}\"\n",
    ")\n",
    "\n",
    "step_1_request = LLMChain(llm=client, prompt=step_1_prompt, output_key=\"name\")\n",
    "\n",
    "# Step 2: Define a prompt for step 2\n",
    "step_2_prompt = PromptTemplate(\n",
    "    input_variables=[\"name\"],\n",
    "    template=\"Does {name} start with letter M?\"\n",
    ")\n",
    "\n",
    "step_2_request = LLMChain(llm=client, prompt=step_2_prompt, output_key=\"answer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "4e54a793-4fe3-45cf-96f1-efb0e9b496f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SequentialChain\n",
    "\n",
    "# Create the chain\n",
    "chain = SequentialChain(\n",
    "    chains=[\n",
    "        step_1_request,  # Step 1\n",
    "        step_2_request   # Step 2\n",
    "    ],\n",
    "    input_variables=[\"input_text\"],\n",
    "    # return_intermediate_steps=True  # Enable intermediate steps\n",
    "    output_variables=[\"name\", \"answer\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "0604c1d3-aaa2-448c-984f-0ad3d449e0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = chain({\"input_text\": \"He is Milad\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "21325a61-cf6d-4ae4-93da-6ae53cef2186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_text': 'He is Milad', 'name': '\\n\\nMilad', 'answer': '\\n\\nYes'}\n"
     ]
    }
   ],
   "source": [
    "print(answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4120f0f9-9b79-4094-b2a1-30b697b6c90b",
   "metadata": {},
   "source": [
    "In LangChain, there is also this **Document Loader** which is a component responsible for loading and processing data from various sources into a format that can be used by the system, typically as text or structured documents. These sources can range from local files (like PDFs, CSVs, or Word documents) to online resources (like web pages, APIs, or databases). In order to be able to implement a document loader, you will have to have **pypdf** library installed and available in your project environment via the command `!pip install pypdf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "6a433fc1-e366-47da-a99c-ba612f035f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(r\"C:\\Users\\ASUS K513EQ\\Downloads\\Mammography Reporting.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "098c4746-798e-4c7e-a95c-3822baab7959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'C:\\\\Users\\\\ASUS K513EQ\\\\Downloads\\\\Mammography Reporting.pdf', 'page': 0, 'page_label': '1'}, page_content='ACR BI-RADS® ATLAS — MAMMOGRAPHY\\nAmerican College of Radiology 121\\nMAMMOGRAPHY\\nII. REPORTING SYSTEM'), Document(metadata={'source': 'C:\\\\Users\\\\ASUS K513EQ\\\\Downloads\\\\Mammography Reporting.pdf', 'page': 1, 'page_label': '2'}, page_content='2013\\n122 American College of Radiology  \\nMAMMOGRAPHY'), Document(metadata={'source': 'C:\\\\Users\\\\ASUS K513EQ\\\\Downloads\\\\Mammography Reporting.pdf', 'page': 2, 'page_label': '3'}, page_content='ACR BI-RADS® ATLAS — MAMMOGRAPHY\\nAmerican College of Radiology 123\\nMAMMOGRAPHY\\nA. REPORT ORGANIZATION ( Guidance chapter, see page 147)\\nThe reporting system should be concise and organized using the following structure. A statement \\nindicating that the current examination has been compared to previous examination(s) should be \\nincluded (specify date[s]). If this is not included, it should be assumed that no comparison has been \\nmade, although it is preferable to indicate that no comparison was made.\\nTable 4. Report Organization\\nReport Structure\\n1. Indication for examination\\n2. Succinct description of the overall breast composition\\n3. Clear description of any important findings\\n4. Comparison to previous examination(s), if deemed appropriate by the interpreting physician\\n5. Assessment\\n6. Management\\n1. INDICATION FOR EXAMINATION\\n Provide a brief description of the indication for examination. This may be screening for an \\nasymptomatic woman, recall of a screening-detected finding, evaluation of a clinical finding \\n(specify the finding and its location), or follow-up of either a probably benign lesion or cancer \\ntreated with breast conservation. If an implant is present, both standard and implant-displaced \\nviews should be performed, and this should be stated in the mammography report.\\n2. SUCCINCT DESCRIPTION OF THE OVERALL BREAST COMPOSITION\\n This is an overall assessment of the volume of attenuating tissues in the breast, to help in-\\ndicate the relative possibility that a lesion could be obscured by normal tissue and that the \\nsensitivity of examination thereby may be compromised by dense breast tissue. A few co-\\nalescent areas of dense tissue may be present in breasts with as little as 10% dense tissue, \\nwhereas primarily fatty areas may be present in breasts with as much as 90% dense tissue.\\n Since mammography does not depict all breast cancers, clinical breast examination is a \\ncomplementary element of screening. Findings at clinical breast examination should not be \\nignored and may have increased importance in the dense breast.\\nThe available data do not support the use of mammographic breast density for determining \\nscreening frequency.\\nThe following four categories of breast composition are defined by the visually estimated con-\\ntent of fibroglandular-density tissue within the breasts. Please note that the categories are listed \\nas a, b, c, and d so as not to be confused with the numbered BI-RADS® assessment categories. \\nIf the breasts are not of apparently equal density, the denser breast should be used to catego-\\nrize breast density. The sensitivity of mammography for noncalcified lesions decreases as the \\nBI-RADS® breast density category increases. The denser the breast, the larger the lesion(s) that \\nmay be obscured. There is considerable intra- and inter-observer variation in visually estimat-\\ning breast density between any two adjacent density categories. Furthermore, there is only'), Document(metadata={'source': 'C:\\\\Users\\\\ASUS K513EQ\\\\Downloads\\\\Mammography Reporting.pdf', 'page': 3, 'page_label': '4'}, page_content='2013\\n124 American College of Radiology  \\nMAMMOGRAPHY\\nTable 5. Breast Tissue\\nBreast Composition Categories\\na. The breasts are almost entirely fatty\\nb. There are scattered areas of fibroglandular density\\nc. The breasts are heterogeneously dense, which may obscure small masses\\nd. The breasts are extremely dense, which lowers the sensitivity of mammography\\na minimal and insignificant difference in the sensitivity of mammography between the densest \\nbreast in a lower-density category and the least dense breast in the next-higher-density category. \\nThese factors limit the clinical relevance of breast density categorization for the individual woman.\\na. The breasts are almost entirely fatty.\\n Unless an area containing cancer is not included in the image field of the mammogram, \\nmammography is highly sensitive in this setting.\\nb. There are scattered areas of fibroglandular density (historically, there are scattered fibro-\\nglandular densities).\\n It may be helpful to distinguish breasts in which there are a few scattered areas of fibroglan-\\ndular-density tissue from those in which there are moderate scattered areas of fibroglandu-\\nlar-density tissue. Note that there has been a subtle change in the wording of this category, \\nto conform to BI-RADS® lexicon use of the term “density” to describe the degree of x-ray \\nattenuation of breast tissue but not to represent discrete mammographic findings.\\nc. The breasts are heterogeneously dense, which may obscure small masses.\\n It is not uncommon for some areas in such breasts to be relatively dense while other areas \\nare primarily fatty. When this occurs, it may be helpful to describe the location(s) of the \\ndenser tissue in a second sentence, so that the referring clinician is aware that these are \\nthe areas in which small noncalcified lesions may be obscured. Suggested wordings for the \\nsecond sentence include:\\n “The dense tissue is located anteriorly in both breasts, and the posterior portions are mostly \\nfatty. ”\\n “Primarily dense tissue is located in the upper outer quadrants of both breasts; scattered \\nareas of fibroglandular tissue are present in the remainder of the breasts. ”\\nd. The breasts are extremely dense, which lowers the sensitivity of mammo graphy.\\n The sensitivity of mammography is lowest in this density category.\\n The fourth edition of BI-RADS®, unlike previous editions, indicated quartile ranges of per-\\ncentage dense tissue (increments of 25% density) for each of the four density categories, \\nwith the expectation that the assignment of breast density would be distributed more \\nevenly across categories than the historical distribution of 10% fatty, 40% scattered, 40% \\nheterogeneously, and 10% extremely dense. However, it has since been demonstrated in'), Document(metadata={'source': 'C:\\\\Users\\\\ASUS K513EQ\\\\Downloads\\\\Mammography Reporting.pdf', 'page': 4, 'page_label': '5'}, page_content='ACR BI-RADS® ATLAS — MAMMOGRAPHY\\nAmerican College of Radiology 125\\nMAMMOGRAPHY\\nFigure 149 –  U.S. Radiologists’ Use of BI-RADS® Breast Density Descriptors, 1996–2008\\n0\\n10\\n1996\\n20\\n30\\nPercentage\\n40\\n50\\n60\\n70\\n80\\nFatty\\n90\\n100\\nScattered areas Heterogeneously dense Extremely dense\\n1997 1998 1999 2000 2001 2002 2003 2004 2005 2006 2007 2008\\nData from 3,865,070 screening mammography examinations interpreted by ra-\\ndiologists who participate in the Breast Cancer Surveillance Consortium (BCSC), \\na group of seven population-based mammography registries covering geo-\\ngraphically, ethnically, and socioeconomically diverse areas of the United States.  \\nData collection for this work was supported by the National Cancer Institute-\\nfunded BCSC cooperative agreement (U01CA63740, U01CA86076, U01CA86082, \\nU01CA63736, U01CA70013, U01CA69976, U01CA63731, U01CA70040).  We \\nthank the BCSC investigators, participating women, mammography facilities, \\nand radiologists for the data they have provided for this study. A list of the BCSC \\ninvestigators and procedures for requesting BCSC data for research purposes are \\nprovided at: http://breastscreening.cancer.gov/.\\nclinical practice that there has been essentially no change in this historical distribution across \\ndensity categories, despite the 2003 guidance provided in the BI-RADS® Atlas (Figure 149).'), Document(metadata={'source': 'C:\\\\Users\\\\ASUS K513EQ\\\\Downloads\\\\Mammography Reporting.pdf', 'page': 5, 'page_label': '6'}, page_content='2013\\n126 American College of Radiology  \\nMAMMOGRAPHY\\n The absence of change in clinical practice of the assignment of breast density across categories \\nmay reflect the reality that a few coalescent areas of dense tissue may be present in breasts with \\nas little as 10% dense tissue, whereas primarily fatty areas may be present in breasts with as much \\nas 90% dense tissue. \\n The fifth edition of BI-RADS® no longer indicates ranges of percentage dense tissue for the four \\ndensity categories. This is done to emphasize the text descriptions of breast density, which reflect \\nthe masking effect of dense fibroglandular tissue on mammographic depiction of noncalcified \\nlesions, because the Committee on BI-RADS® concludes that the association of subjectively es-\\ntimated breast density with changes in the sensitivity of mammography is clinically more im-\\nportant than the relatively smaller effect of percentage breast density as an indicator for breast \\ncancer risk.\\n The Committee on BI-RADS® indeed is aware of recent and continuing investigations of percent-\\nage breast density as an indicator for breast cancer risk, and by eliminating percentage ranges we \\ndo not intend to compromise or impede any such research. We simply recognize the reality that \\ninterpreting physicians will continue to use density categories in mammography reports as they \\nhave done over the past many years, independent of BI-RADS® guidance on percentage breast \\ndensity. We further recognize that both subjective estimates and planimetry measurements of \\nbreast density based on area as depicted on (2-D) mammograms are imprecise indicators of \\nthe volume of dense tissue, which may be measured using (3-D) cross-sectional breast imaging \\nmodalities.1 We await publication of robust volume-based breast density data, using validated \\npercentage cut points (not necessarily quartiles) that are readily and reproducibly determined \\nat imaging, before again indicating percentage ranges for BI-RADS® density categories. We also \\nurge avoidance of numbers to classify breast density instead of BI-RADS® terminology in order \\nto avoid confusion with BI-RADS® assessment categories, which are numbered.\\n Some breasts may appear more or less dense when imaged using full-field digital mammog-\\nraphy compared to screen-film mammography. Superior depiction of the skin line by digital \\nmammography provides the observer with a more accurate (and usually larger) estimate of \\nthe extent of the subcutaneous fat. However, no change in the distribution across density cat-\\negories has been observed when comparing full-field digital mammography with screen-film \\nmammography.2'), Document(metadata={'source': 'C:\\\\Users\\\\ASUS K513EQ\\\\Downloads\\\\Mammography Reporting.pdf', 'page': 6, 'page_label': '7'}, page_content='ACR BI-RADS® ATLAS — MAMMOGRAPHY\\nAmerican College of Radiology 127\\nMAMMOGRAPHY\\nBREAST COMPOSITION ILLUSTRATIONS'), Document(metadata={'source': 'C:\\\\Users\\\\ASUS K513EQ\\\\Downloads\\\\Mammography Reporting.pdf', 'page': 7, 'page_label': '8'}, page_content='2013\\n128 American College of Radiology  \\nMAMMOGRAPHY\\nBREAST COMPOSITION ILLUSTRATIONS\\n a. The breasts are almost entirely fatty.\\nFigure 150 — The breasts are almost entirely fatty.'), Document(metadata={'source': 'C:\\\\Users\\\\ASUS K513EQ\\\\Downloads\\\\Mammography Reporting.pdf', 'page': 8, 'page_label': '9'}, page_content='ACR BI-RADS® ATLAS — MAMMOGRAPHY\\nAmerican College of Radiology 129\\nMAMMOGRAPHY\\nBREAST COMPOSITION ILLUSTRATIONS\\n b. There are scattered areas of fibroglandular density.\\nFigure 151— There are scattered areas of fibroglandular density.'), Document(metadata={'source': 'C:\\\\Users\\\\ASUS K513EQ\\\\Downloads\\\\Mammography Reporting.pdf', 'page': 9, 'page_label': '10'}, page_content='2013\\n130 American College of Radiology  \\nMAMMOGRAPHY\\nBREAST COMPOSITION ILLUSTRATIONS\\n c. The breasts are heterogeneously dense, which may obscure small masses.\\nFigure 152 — The breasts are heterogeneously dense, which may obscure small masses.'), Document(metadata={'source': 'C:\\\\Users\\\\ASUS K513EQ\\\\Downloads\\\\Mammography Reporting.pdf', 'page': 10, 'page_label': '11'}, page_content='ACR BI-RADS® ATLAS — MAMMOGRAPHY\\nAmerican College of Radiology 131\\nMAMMOGRAPHY\\nFigure 153 — Breast density is classified using the denser breast. In this case, because the fibro-\\nglandular tissue in the upper outer right breast is sufficiently dense to obscure small masses, the \\nexamination should be classified as HETEROGENEOUSLY DENSE, even though far less than 50% of \\nthe volume of this (denser) breast contains fibroglandular-density tissue.'), Document(metadata={'source': 'C:\\\\Users\\\\ASUS K513EQ\\\\Downloads\\\\Mammography Reporting.pdf', 'page': 11, 'page_label': '12'}, page_content='2013\\n132 American College of Radiology  \\nMAMMOGRAPHY\\nBREAST COMPOSITION ILLUSTRATIONS\\n d. The breasts are extremely dense, which lowers the sensitivity of mammography.\\nFigure 154 — The breasts are extremely dense, which lowers the sensitivity \\nof mammography.'), Document(metadata={'source': 'C:\\\\Users\\\\ASUS K513EQ\\\\Downloads\\\\Mammography Reporting.pdf', 'page': 12, 'page_label': '13'}, page_content='ACR BI-RADS® ATLAS — MAMMOGRAPHY\\nAmerican College of Radiology 133\\nMAMMOGRAPHY\\n3. CLEAR DESCRIPTION OF ANY IMPORTANT FINDINGS\\n  (It is assumed that most important findings are either of concern at screening, inherently suspi-\\ncious, new, or seen to be larger/more extensive when compared to previous examination.)\\na. Mass:\\n Size\\n Morphology (shape, margin)\\n Density\\n Associated calcifications\\n Associated features\\n Location\\nb. Calcifications:\\n Morphology — describe typically benign type or describe shape of particles\\n Distribution (may not be appropriate for typically benign calcifications)\\n Associated features\\n Location\\nc. Architectural Distortion:\\n Associated calcifications\\n Associated features\\n Location\\nd. Asymmetries (asymmetry, global asymmetry, focal asymmetry, developing asymmetry): \\n Associated calcifications\\n Associated features\\n Location\\ne. Intramammary lymph node (rarely important):\\n Location\\nf. Skin lesion (rarely important): \\n Location\\ng. Solitary dilated duct (rarely present):\\n Location'), Document(metadata={'source': 'C:\\\\Users\\\\ASUS K513EQ\\\\Downloads\\\\Mammography Reporting.pdf', 'page': 13, 'page_label': '14'}, page_content='2013\\n134 American College of Radiology  \\nMAMMOGRAPHY\\n4. COMPARISON TO PREVIOUS EXAMINATION(S), IF DEEMED APPROPRIATE BY THE INTERPRETING \\nPHYSICIAN\\n  Comparison to previous examination may assume importance if the finding of concern requires \\nan evaluation of change or stability. Comparison is not important when a finding has unequivo-\\ncally benign features. Comparison may be irrelevant when the finding is inherently suspicious \\nfor malignancy.\\n5. ASSESSMENT\\n  The incorporation of an assessment category in the overall summary of the mammography report \\nis mandated by the Food and Drug Administration, Quality Mammography Standards; Final Rule.3 \\nWhereas FDA-mandated assessments are not linked to management recommendations, BI-RADS® \\nassessment categories are designed to be concordant with specific management recommenda-\\ntions. The linking of assessment categories with concordant management recommendations \\nfurther enhances sound medical practice.\\n  All final assessments (BI-RADS® categories 1, 2, 3, 4, 5, and 6) should be based on thorough \\nevaluation of the mammographic features of concern or after determination that an examina-\\ntion is negative or benign.\\n  An incomplete (category 0) assessment is usually given for screening examinations when ad-\\nditional imaging evaluation is recommended before it is appropriate to render a final assess-\\nment. There may be rare situations in the screening setting in which a category 4 or 5 assess-\\nment is used, but this practice is discouraged because it may compromise some aspects of \\noutcome analysis.\\n  A recall (category 0) assessment should include specific suggestions for the next course of action \\n(spot-compression magnification views, US, etc.).\\n6. MANAGEMENT\\n  If a suspicious abnormality is identified, the report should indicate that a biopsy should be per-\\nformed in the absence of clinical contraindication. This is an assessment for which the interpret-\\ning physician has sufficient concern that biopsy is warranted based on imaging findings. The \\nrecommended language (“biopsy should be performed in the absence of clinical contraindica-\\ntion”) provides for the unusual circumstance in which either the patient or her physician might \\nreasonably wish to defer a biopsy.\\nInterpretation is facilitated by recognizing that most examinations may be categorized under a few \\nheadings. These are listed in Table 6 and numeric codes are included for computer use.'), Document(metadata={'source': 'C:\\\\Users\\\\ASUS K513EQ\\\\Downloads\\\\Mammography Reporting.pdf', 'page': 14, 'page_label': '15'}, page_content='ACR BI-RADS® ATLAS — MAMMOGRAPHY\\nAmerican College of Radiology 135\\nMAMMOGRAPHY\\nB.  ASSESSMENT CATEGORIES\\nTable 6.  Concordance Between BI-RADS® Assessment Categories and Management Recommendations\\nAssessment Management Likelihood of Cancer\\nCategory 0: Incomplete – Need  \\nAdditional Imaging Evaluation and/or \\nPrior Mammograms for Comparison\\nRecall for additional  \\nimaging and/or comparison \\nwith prior examination(s)\\nN/A\\nCategory 1: Negative Routine mammography \\nscreening\\nEssentially 0% likelihood of malignancy\\nCategory 2: Benign Routine mammography \\nscreening\\nEssentially 0% likelihood of malignancy\\nCategory 3: Probably BenignShort-interval (6-month) \\nfollow-up or continued \\nsurveillance mammography \\n(Figure 155, see page 152)\\n> 0% but ≤ 2% likelihood of malignancy\\nCategory 4: Suspicious\\nCategory 4A: Low suspicion for \\nmalignancy\\nCategory 4B: Moderate suspicion for  \\nmalignancy\\nCategory 4C: High suspicion for  \\nmalignancy\\nTissue diagnosis > 2% but < 95% likelihood of malignancy\\n> 2% to ≤ 10% likelihood of malignancy\\n> 10% to ≤ 50% likelihood of  \\nmalignancy\\n> 50% to < 95% likelihood of  \\nmalignancy\\nCategory 5: Highly Suggestive of  \\nMalignancy\\nTissue diagnosis ≥ 95% likelihood of malignancy\\nCategory 6: Known Biopsy-Proven \\nMalignancy\\nSurgical excision when  \\nclinically appropriate\\nN/A\\na. Mammographic Assessment Is Incomplete\\nCategory 0: Incomplete — Need Additional Imaging Evaluation and/or Prior Mammograms \\nfor Comparison\\n  For this assessment category, the text may be shortened to “Incomplete — Need Additional Imag-\\ning Evaluation” or “Incomplete — Need Prior Mammograms for Comparison” , as appropriate. Refer \\nto Table 9 (see page 168) in Frequently Asked Question #1 in the Guidance chapter for a listing of \\nFDA-approved equivalent wording for assessment categories.\\n  There is a finding for which additional imaging evaluation is needed. This is almost always used in a \\nscreening situation. Under certain circumstances this assessment category may be used in a diagnos-\\ntic mammography report, such as when US equipment or personnel are not immediately available, \\nor when the patient is unable or unwilling to wait for completion of a full diagnostic examination. A \\nrecommendation for additional imaging evaluation includes the use of spot-compression (with or \\nwithout magnification), special mammographic views, and US. Category 0 should not be used for \\ndiagnostic breast imaging findings that warrant further evaluation with MRI. Rather, the interpreting \\nphysician should issue a final assessment in a report that is made before the MRI examination is per-\\nformed. Refer to Frequently Asked Question #8 (See page 162) in the Guidance chapter for further \\ndiscussion.'), Document(metadata={'source': 'C:\\\\Users\\\\ASUS K513EQ\\\\Downloads\\\\Mammography Reporting.pdf', 'page': 15, 'page_label': '16'}, page_content='2013\\n136 American College of Radiology  \\nMAMMOGRAPHY\\n  In most circumstances and when feasible, if a mammography examination is not assessed as \\nnegative or benign, the current examination should be compared with prior examination(s). The \\ninterpreting physician should use judgment on how vigorously to attempt obtaining prior exami-\\nnations, given the likelihood of success of such an endeavor and the likelihood that comparison \\nwill affect the final assessment. In this context, it is important to note that comparison with previ-\\nous examination(s) may be irrelevant when a finding is inherently suspicious for malignancy. \\n  Category 0 should be used for prior image comparison only when such comparison is required to \\nmake a final assessment. When category 0 is used in the context of awaiting prior examinations \\nfor comparison, there should be in place a tracking procedure guaranteeing with 100% reliability \\nthat a final assessment will be made within 30 days (preferably sooner) even if prior examinations \\ndo not become available. Some mammography practices may reasonably choose never to use \\ncategory 0 in the context of awaiting prior examinations simply because they do not have a 100% \\nreliable tracking procedure. If a mammography examination is assessed as category 0 in the con-\\ntext of awaiting prior examinations and then the prior examinations do become available, an ad-\\ndendum to the initial mammography report should be issued, including a revised assessment. For \\nauditing purposes, the revised assessment should replace the initial assessment (see the Follow-\\nup and Outcome Monitoring section).\\nb.  Mammographic Assessment Is Complete — Final Assessment Categories\\n  Category 1: Negative (see Guidance chapter)\\n  There is nothing to comment on. This is a normal examination.\\n  Category 2: Benign (see Guidance chapter)\\n  Like category 1, this is a normal assessment, but here the interpreter chooses to describe a be-\\nnign finding in the mammography report. Involuting calcified fibroadenomas, skin calcifications, \\nmetallic foreign bodies (such as core biopsy and surgical clips), and fat-containing lesions (such \\nas oil cysts, lipomas, galactoceles, and mixed-density hamartomas) all have characteristically be-\\nnign appearances and may be described with confidence. The interpreter may also choose to \\ndescribe intramammary lymph nodes, vascular calcification, implants, or architectural distortion \\nclearly related to prior surgery while still concluding that there is no mammographic evidence \\nof malignancy. On the other hand, the interpreter may choose not to describe such findings, in \\nwhich case the examination should be assessed as negative (category 1).\\n  Note that both category 1 and category 2 assessments indicate that there is no mammographic \\nevidence of malignancy. Both should be followed by the management recommendation for rou-\\ntine mammography screening. The difference is that category 2 should be used when describing \\none or more specific benign mammographic findings in the report, whereas category 1 should \\nbe used when no such findings are described (even if such findings are present).\\n Category 3: Probably Benign (see Guidance chapter, including Figure 155)\\n  A finding assessed using this category should have a ≤ 2% likelihood of malignancy, but greater \\nthan the essentially 0% likelihood of malignancy of a characteristically benign finding. A prob-\\nably benign finding is not expected to change over the suggested period of imaging surveil-\\nlance, but the interpreting physician prefers to establish stability of the finding before recom-\\nmending management limited to routine mammography screening.'), Document(metadata={'source': 'C:\\\\Users\\\\ASUS K513EQ\\\\Downloads\\\\Mammography Reporting.pdf', 'page': 16, 'page_label': '17'}, page_content='ACR BI-RADS® ATLAS — MAMMOGRAPHY\\nAmerican College of Radiology 137\\nMAMMOGRAPHY\\n  There are several prospective clinical studies demonstrating the safety and efficacy of periodic \\nmammographic surveillance instead of biopsy for specific mammographic findings.4–9 Three \\nspecific findings are validated as being probably benign (noncalcified circumscribed solid mass, \\nfocal asymmetry, and solitary group of punctate calcifications). All the previously cited studies \\nemphasize the need to conduct a complete diagnostic imaging evaluation before making a \\nprobably benign (category 3) assessment; hence, it is recommended not to render such an as-\\nsessment in interpreting a screening mammography examination. The practice of rendering \\ncategory 3 assessments directly from screening examination also has been shown to result in \\nadverse outcomes: 1) unnecessary follow-up of many lesions that could have been promptly \\nassessed as benign, and 2) delayed diagnosis of a small number of cancers that otherwise may \\nhave been smaller in size and less likely to be advanced in stage.10 Also, all the previously cited \\nstudies4–9 exclude palpable lesions, so the use of a probably benign assessment for a palpable \\nlesion is not supported by robust scientific data, although there are two single-institution studies \\nthat do report successful outcomes for palpable lesions.11,12 Finally, because evidence from pre-\\nviously cited studies indicates the need for biopsy rather than continued surveillance when a \\nprobably benign finding increases in size or extent,4–9 it is not prudent to render a category 3 as-\\nsessment when a finding that otherwise meets “probably benign” imaging criteria is either new \\nor has increased in size or extent.\\n  Refer to Figure 155 (see page 152) at the end of the Guidance chapter for an illustration of the rec-\\nommended algorithm for follow-up examinations during the entire mammographic surveillance \\nperiod. While the vast majority of probably benign findings are managed with an initial short-\\ninterval follow-up (6 months) examination followed by additional examinations until long-term \\n(2- or 3-year) stability is demonstrated, there may be occasions in which a biopsy is done instead \\n(patient preference or overriding clinical concern).\\n  Category 4: Suspicious ( Guidance chapter, see page 153)\\n  This category is reserved for findings that do not have the classic appearance of malignancy but \\nare sufficiently suspicious to justify a recommendation for biopsy. The ceiling for category 3 \\nassessment is a 2% likelihood of malignancy and the floor for category 5 assessment is 95%, \\nso category 4 assessments cover the wide range of likelihood of malignancy in between. Thus, \\nalmost all recommendations for breast interventional procedures will come from assessments \\nmade using this category. By subdividing category 4 into 4A, 4B, and 4C, as recommended in \\nGuidance chapter and using the cut points indicated therein, it is hoped that patients and refer-\\nring clinicians will more readily make informed decisions on the ultimate course of action.\\n  Category 5: Highly Suggestive of Malignancy ( Guidance chapter, see page 154)\\n  These assessments carry a very high probability (≥ 95%) of malignancy. This category initially \\nwas established to involve lesions for which 1-stage surgical treatment was considered without \\npreliminary biopsy, in an era when preoperative wire localization was the primary breast inter-\\nventional procedure. Nowadays, given the widespread acceptance of imaging-guided percuta-\\nneous biopsy, 1-stage surgery is rarely, if ever, performed. Rather, current oncologic management \\nalmost always involves tissue diagnosis of malignancy via percutaneous tissue sampling to facili-\\ntate treatment options, such as when sentinel node biopsy is included in surgical management \\nor when neoadjuvant chemotherapy is administered prior to surgery. Therefore, the current ra-'), Document(metadata={'source': 'C:\\\\Users\\\\ASUS K513EQ\\\\Downloads\\\\Mammography Reporting.pdf', 'page': 16, 'page_label': '17'}, page_content='tate treatment options, such as when sentinel node biopsy is included in surgical management \\nor when neoadjuvant chemotherapy is administered prior to surgery. Therefore, the current ra-\\ntionale for using a category 5 assessment is to identify lesions for which any non-malignant'), Document(metadata={'source': 'C:\\\\Users\\\\ASUS K513EQ\\\\Downloads\\\\Mammography Reporting.pdf', 'page': 17, 'page_label': '18'}, page_content='2013\\n138 American College of Radiology  \\nMAMMOGRAPHY\\npercutaneous tissue diagnosis is automatically considered discordant, resulting in the recom-\\nmendation for repeat (usually surgical) biopsy.\\n  Category 6: Known Biopsy-Proven Malignancy (Guidance chapter, see page 155):\\n  This category is reserved for examinations performed after biopsy proof of malignancy (imaging \\nperformed after percutaneous biopsy but prior to complete surgical excision), in which there are no \\nmammographic abnormalities other than the known cancer that might need additional evaluation.'), Document(metadata={'source': 'C:\\\\Users\\\\ASUS K513EQ\\\\Downloads\\\\Mammography Reporting.pdf', 'page': 18, 'page_label': '19'}, page_content='ACR BI-RADS® ATLAS — MAMMOGRAPHY\\nAmerican College of Radiology 139\\nMAMMOGRAPHY\\nC. WORDING THE REPORT\\nThe current examination should be COMPARED TO PRIOR EXAMINATION(S) when appropriate. The \\nINDICATION FOR EXAMINATION, such as screening or diagnostic, should be stated. The report should \\nbe organized with a brief description of the COMPOSITION of the breast and any pertinent FINDINGS, \\nfollowed by the ASSESSMENT and MANAGEMENT RECOMMENDATIONS. Any verbal discussions be -\\ntween the interpreting physician and the referring clinician or patient should be documented in \\nthe original report or in an addendum to the report.\\nThe report should be succinct, using terminology from the latest approved lexicon without embel-\\nlishment. Definitions of lexicon terms for mammographic findings should not appear in the report \\nnarrative. Following the impression section and the (concordant) management recommendation \\nsection of the report, both the assessment category number and FDA-approved terminology for the \\nassessment category should be stated. Other aspects of the report should comply with the ACR \\nPractice Guideline for Communication: Diagnostic Radiology.13'), Document(metadata={'source': 'C:\\\\Users\\\\ASUS K513EQ\\\\Downloads\\\\Mammography Reporting.pdf', 'page': 19, 'page_label': '20'}, page_content='2013\\n140 American College of Radiology  \\nMAMMOGRAPHY\\nREFERENCES\\n 1. Kopans DB. Basic physics and doubts about relationship between mammographically determined tissue den-\\nsity and breast cancer risk. Radiology 2008; 246(2):348–353.\\n 2.  Harvey JA, Gard CC, Miglioretti DL, et al. Reported mammographic density: film-screen versus digital acquisi-\\ntion. Radiology 2013; 266(3):752–758.\\n 3. 21CFR Part 16 and 900: Mammography Quality Standards; Final Rule. Federal Register, Washington, DC: \\nGovernment Printing Office, 62: No. 208; 55851-55994, October 28, 1997.\\n 4. Sickles EA. Periodic mammographic follow-up of probably benign lesions: results in 3,184 consecutive cases. \\nRadiology 1991;179(2):463–468\\n 5. Varas X, Leborgne F, Leborgne JH. Nonpalpable, probably benign lesions: role of follow-up mammography. \\nRadiology 1992;184(2):409–414.\\n 6. Wolfe JN, Buck KA, Salane M, Parekh NJ. Xeroradiography of the breast: overview of 21, 057 consecutive cases. \\nRadiology 1987;165(2):305–311.\\n 7. Helvie MA, Pennes DR, Rebner M, Adler DD. Mammographic follow-up of low-suspicion lesions: compliance \\nrate and diagnostic yield. Radiology 1991; 178(1):155–158.\\n 8. Vizcaíno I, Gadea L, Andreo L, et al. Short-term follow-up results in 795 nonpalpable probably benign lesions \\ndetected at screening mammography. Radiology 2001;219(2):475–483.\\n 9. Varas X, Leborgne JH, Leborgne F, Mezzera J, Jaumandreu S, Leborgne F. Revisiting the mammographic follow-\\nup of BI-RADS category 3 lesions. AJR 2002; 179(3):691–695.\\n 10. Kerlikowske K, Smith-Bindman R, Abraham LA, et al. Breast cancer yield for screening mammographic exami-\\nnations with recommendation for short-interval follow-up. Radiology 2005; 234(3):684–692.\\n 11. Graf O, Helbich TH, Fuchsjaeger MH, et al. Follow-up of palpable circumscribed noncalcified solid breast masses \\nat mammography and US: can biopsy be averted? Radiology 2004; 233(3):850–856.\\n 12. Harvey JA, Nicholson BT, LoRusso AP , Cohen MA, Bovbjerg VE. Short-term follow-up of palpable breast lesions \\nwith benign imaging features: evaluation of 375 lesions in 320 women. AJR 2009; 193(3):1723–1730.\\n 13. American College of Radiology. ACR Practice Guideline for Communication: Diagnostic Radiology. http://www.\\nacr.org/~/media/ACR/Documents/PGTS/guidelines/Comm_Diag_Imaging.pdf. Accessed November 4, 2013.')]\n"
     ]
    }
   ],
   "source": [
    "pages = loader.load_and_split()\n",
    "print(pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc7737d-b76f-4908-9060-118017c08ccf",
   "metadata": {},
   "source": [
    "#### Why are document loaders important?\n",
    "Document loaders play a crucial role in automating workflows, as they enable LangChain to interact seamlessly with various data sources. For instance:\n",
    "\n",
    "- In a legal document analysis application, a loader can extract and preprocess contracts from a folder of PDFs.\n",
    "- For a question-answering system, a loader can retrieve articles from the web and split them into chunks for embedding and retrieval."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce90ef8-247c-4127-be48-a4fe8f713758",
   "metadata": {},
   "source": [
    "### Memory\n",
    "LLMs like ChatGPT simulate memory within a single conversation by maintaining a context window that stores previous prompts and responses. This allows the model to generate coherent replies by referring back to earlier parts of the conversation. However, this \"memory\" is session-specific and limited by the model's context length (e.g., GPT-4 has a maximum token limit).\n",
    "\n",
    "Outside a session, true memory doesn't exist unless explicitly implemented (e.g., external memory or logging via APIs). This differs from human-like memory, as it’s limited to processing textual patterns and lacks long-term storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "015edc8a-02d9-4f0c-aeaf-648d70b52c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b74b651f-a3a1-4add-b3d5-a54256ba13be",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_lang = PromptTemplate(\n",
    "    input_variables = [\"language\"],\n",
    "    template = \"What is a country in Europe that speaks {language}?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ef02d8fb-0358-498a-a627-236103b44804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create client\n",
    "client = OpenAI(openai_api_key=my_openaiapi_key, temperature=1.0)\n",
    "\n",
    "# Create your language request\n",
    "country_request = LLMChain(llm=client, prompt=prompt_template_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a82fd217-3a51-445d-8872-94c1c334eb51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Ireland is a country in Europe where English is the primary language spoken. It is the only country in Europe with English as its official language. Other countries in Europe may have English as a secondary or widely spoken language, but it is not their official language.\n"
     ]
    }
   ],
   "source": [
    "print(country_request.run(\"English\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f11a10-cdc6-40dd-9530-a6a22debe23e",
   "metadata": {},
   "source": [
    "**ConversationBufferMemory** is a concept used in frameworks like LangChain to mimic memory for LLMs during a conversation. It involves storing the history of the dialogue (prompts and responses) in a buffer and providing this history as part of the input to the model in each turn. This ensures the model has context from the entire conversation.\n",
    "\n",
    "While LLMs themselves don’t have inherent long-term memory, ConversationBufferMemory helps manage and feed the conversation history systematically, enabling the model to generate coherent responses by \"remembering\" earlier parts of the dialogue. This approach is especially useful when building conversational agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d7d8d306-5f02-426b-bdbc-4dc042f3e8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# Define the memory and the LLM\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "# Create a conversation chain with memory\n",
    "country_request = LLMChain(llm=client, prompt=prompt_template_lang, memory=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d104dd-59a4-4545-a6d9-6c8507778088",
   "metadata": {},
   "source": [
    "Now let's trigger a chat between the *User* and *AI* passing several keywords to form a conversational thread:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "67237ba8-fc03-46e8-a2bb-4791df624d55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nThe Netherlands is a country in Europe that speaks Dutch.'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "country_request.run(\"Dutch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "80cf6506-4b09-4d75-aae3-18e8c8703ded",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nGermany is the main country in Europe that speaks German. Austria and Switzerland also have significant populations who speak German, and it is recognized as an official language in Liechtenstein, Luxembourg, and Belgium. Parts of Italy, France, and Poland also have German-speaking communities.'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "country_request.run(\"German\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3599e630-51a4-47d0-8a7d-15ee2118b546",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nIreland is the only country in Europe where Irish (Gaeilge) is recognized as an official language and is widely spoken.'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "country_request.run(\"Irish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a3a40e6d-ff71-4589-8660-ab7c5970af7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConversationBufferMemory(chat_memory=InMemoryChatMessageHistory(messages=[HumanMessage(content='Dutch', additional_kwargs={}, response_metadata={}), AIMessage(content='\\n\\nThe Netherlands is a country in Europe that speaks Dutch.', additional_kwargs={}, response_metadata={}), HumanMessage(content='German', additional_kwargs={}, response_metadata={}), AIMessage(content='\\n\\nGermany is the main country in Europe that speaks German. Austria and Switzerland also have significant populations who speak German, and it is recognized as an official language in Liechtenstein, Luxembourg, and Belgium. Parts of Italy, France, and Poland also have German-speaking communities.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Irish', additional_kwargs={}, response_metadata={}), AIMessage(content='\\n\\nIreland is the only country in Europe where Irish (Gaeilge) is recognized as an official language and is widely spoken.', additional_kwargs={}, response_metadata={})]))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Access the memory\n",
    "country_request.memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dc01c9f4-ded5-423e-be25-6764ceb5c22f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human: Dutch\\nAI: \\n\\nThe Netherlands is a country in Europe that speaks Dutch.\\nHuman: German\\nAI: \\n\\nGermany is the main country in Europe that speaks German. Austria and Switzerland also have significant populations who speak German, and it is recognized as an official language in Liechtenstein, Luxembourg, and Belgium. Parts of Italy, France, and Poland also have German-speaking communities.\\nHuman: Irish\\nAI: \\n\\nIreland is the only country in Europe where Irish (Gaeilge) is recognized as an official language and is widely spoken.'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Access the buffer where the chat content stored\n",
    "country_request.memory.buffer.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670c06e9-f3ab-4bac-87cb-28cca3231d6e",
   "metadata": {},
   "source": [
    "Below you can see the conversation taken place between the *User* and *AI*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "604531ee-1203-4bf3-ac00-fa06cd9fc7bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Dutch\n",
      "AI: \n",
      "\n",
      "The Netherlands is a country in Europe that speaks Dutch.\n",
      "Human: German\n",
      "AI: \n",
      "\n",
      "Germany is the main country in Europe that speaks German. Austria and Switzerland also have significant populations who speak German, and it is recognized as an official language in Liechtenstein, Luxembourg, and Belgium. Parts of Italy, France, and Poland also have German-speaking communities.\n",
      "Human: Irish\n",
      "AI: \n",
      "\n",
      "Ireland is the only country in Europe where Irish (Gaeilge) is recognized as an official language and is widely spoken.\n"
     ]
    }
   ],
   "source": [
    "print(country_request.memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3b4584-a83e-478c-b348-9624f24b4679",
   "metadata": {},
   "source": [
    "**ConversationChain** in LangChain builds on this concept of memory. It uses a memory module to *store* and *manage* the conversational context explicitly, enabling the model to \"remember\" previous exchanges within the same interaction.\n",
    "\n",
    "While ChatGPT inherently maintains context within its session, a ConversationChain extends this functionality by making memory handling explicit and customizable. For example:\n",
    "\n",
    "- **BufferMemory** stores all conversation history (like ChatGPT's session context).\n",
    "- **SummaryMemory** summarizes past interactions into a concise format to save space for longer conversations.\n",
    "- **EntityMemory** tracks specific entities mentioned in the conversation.\n",
    "These mechanisms make ConversationChain a practical tool for building applications where maintaining dynamic context is crucial, such as customer support or dialogue systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8008a33a-a137-4378-816e-bb126f46dd01",
   "metadata": {},
   "source": [
    "According to [medium.com](https://medium.com/@RSK2327/breaking-down-langchain-chatopenai-and-conversationchain-03565f421f78), ConversationChain is a subclass of LLMChain thats meant to be used specifically for chat agent applications. However, most of the functions being used are actually being implemented in the LLMChain and its parent, Chain, class. So we can reproduce the same results by just using the LLMChain class. Yet using using ConversationChain recommended as it implements certain checks to ensure the workflow doesnt break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e80a831-3f7a-4648-b7c2-0d923b3a74e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ef7bc6c-ead3-4770-b5e0-cceceab23681",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1 = ConversationChain(llm=client)\n",
    "# conv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9383a10d-6071-425d-a4a0-1a72b619103b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(conv1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "12a6ab41-ad05-4e4f-b1a7-50c9aefe5e4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PromptTemplate(input_variables=['history', 'input'], input_types={}, partial_variables={}, template='The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\n\\nCurrent conversation:\\n{history}\\nHuman: {input}\\nAI:'),\n",
       " 'The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\n\\nCurrent conversation:\\n{history}\\nHuman: {input}\\nAI:')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1.prompt, conv1.prompt.template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b7d8114b-9d7d-46ee-9899-fb9b29c15a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "{history}\n",
      "Human: {input}\n",
      "AI:\n"
     ]
    }
   ],
   "source": [
    "print(conv1.prompt.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ac1245-5ded-4136-b73d-f24cc07782df",
   "metadata": {},
   "source": [
    "**BINGO!** Conversation Chain is created and ready to use. Lwt us make a few requests!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "52060631-4388-4baa-a5bd-11e788a11213",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Abraham Lincoln was born on February 12, 1809 in a one-room log cabin in Hardin County, Kentucky. His parents were Thomas Lincoln and Nancy Hanks Lincoln. He was the second child of the couple, with his older sister Sarah being two years older.'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1.run(\"When was Abraham Lincoln born?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2dbcf245-05d9-45fa-b3c2-7d64ef6624bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The capital of China is Beijing.'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1.run(\"In a short answer, what is the capital of China\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "12d549e8-128a-4e79-afc1-78b3ddac3f4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' In a single word, London is bigger than Beijing.'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1.run(\"In a single word, which is bigger, Beijing or London?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "900e8d82-ed91-47c2-9633-fed69a3da693",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Abraham Lincoln's full name was actually Abraham Lincoln, as he did not have a middle name. His parents were reportedly illiterate and did not know how to properly register his birth, so he was simply given his first and last name.\""
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1.run(\"What was his full name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d3765dc9-98e5-47f1-9630-8bb2d2dcf0f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: When was Abraham Lincoln born?\n",
      "AI:  Abraham Lincoln was born on February 12, 1809 in a one-room log cabin in Hardin County, Kentucky. His parents were Thomas Lincoln and Nancy Hanks Lincoln. He was the second child of the couple, with his older sister Sarah being two years older.\n",
      "Human: Briefly, what color is this piece of text?\n",
      "AI:  This piece of text appears to be black. However, it is important to note that the color of text can vary depending on the typeface, font size, and background color it is displayed on. For example, if this text was enlarged or displayed on a white background, it may appear slightly different shades of gray.\n",
      "Human: In a short answer, what is the capital of China\n",
      "AI:  The capital of China is Beijing.\n",
      "Human: In a single word, which is bigger, Beijing or London?\n",
      "AI:  In a single word, London is bigger than Beijing.\n",
      "Human: What was his full name?\n",
      "AI:  Abraham Lincoln's full name was actually Abraham Lincoln, as he did not have a middle name. His parents were reportedly illiterate and did not know how to properly register his birth, so he was simply given his first and last name.\n"
     ]
    }
   ],
   "source": [
    "print(conv1.memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05e303b-7129-48d6-8d30-16c1eb4f618b",
   "metadata": {},
   "source": [
    "**ConversationBufferWindowMemory** is a specific type of memory module in LangChain that keeps track of a fixed-size sliding window of the most recent interactions in a conversation. It doesn't store the entire history like ConversationBufferMemory, but rather focuses on the last n messages.\n",
    "\n",
    "Key Characteristics:\n",
    "- Windowed Context: It remembers only a limited number of recent exchanges (e.g., the last 5 turns).\n",
    "- Efficiency: By limiting the stored context, it avoids memory overload and keeps the computational load manageable, especially for long conversations.\n",
    "Dynamic Updates: As new messages are added, older ones fall out of the window.\n",
    "\n",
    "Use Case:\n",
    "- This memory type is particularly useful in scenarios where:\n",
    "- Short-Term Relevance: Only recent interactions matter for the conversation.\n",
    "Efficiency: You need to save resources while maintaining relevant context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2e3c4e1b-0895-49d1-a7a6-4d3fcbfbd0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "bd450f23-2efb-4324-9f1f-4b331df664fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the memory with a window size of 3 (remembers the last 3 interactions)\n",
    "memory2 = ConversationBufferWindowMemory(k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7eb84975-d82e-490e-a4de-66a8af133073",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv2 = ConversationChain(llm=OpenAI(openai_api_key=my_openaiapi_key, temperature=1.0), \n",
    "                          memory=memory2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "18d71656-2c00-4549-a614-71f1cbe5d6d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The capital of Iran is Tehran, a bustling metropolis with a rich history and a population of over 8 million people, located in the northern part of the country at the foot of the Alborz Mountains.'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2.run(\"In a single sentence, what is the capital of Iran?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "cb5f8671-011c-43f3-b8d1-ca10df505d63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 2+2 is equal to 4, a basic mathematical operation that involves adding two numbers together.'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2.run(\"What is 2+2?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b7b20564-a046-453b-8b61-bcc4fc0e6013",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' I would need more context to answer that question accurately. What is the \"it\" you are referring to? Is it a specific location or a general area?'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2.run(\"What is the closest city to it?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9bfd55-dcf6-46e8-878f-ef6b5ebf34f1",
   "metadata": {},
   "source": [
    "It is clearly noticeable that the model fails to provide a reliable answer since it has no clue what \"it\" refers to. We have set `k=1` and therefore the longest it can look back is for 1 single message. In other words, it can only remember one last response. To explore further about memory, you can read [LangChain Documentation](https://python.langchain.com/v0.1/docs/modules/memory/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e1ee89-abf6-4ee8-ab1e-cd26b951410d",
   "metadata": {},
   "source": [
    "Hope you enjoyed the brief tutorial!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13430ccb-6c48-4380-ba3f-50008b70161e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
