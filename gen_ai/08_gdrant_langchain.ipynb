{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Qdrant\n",
        "Qdrant is an open-source vector database optimized for storing and querying high-dimensional vector embeddings, commonly used in machine learning and AI applications‚Äîespecially those involving semantic search, recommendation systems, and retrieval-augmented generation (RAG)."
      ],
      "metadata": {
        "id": "Vg6R4-O55PvG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üöÄ What is Qdrant?\n",
        "At its core, Qdrant (Quantum Data Rant) is a high-performance vector similarity search engine that:\n",
        "\n",
        "- Stores data as vectors (like sentence embeddings or image embeddings)\n",
        "\n",
        "- Enables fast nearest neighbor search based on vector similarity\n",
        "\n",
        "- Supports hybrid search (vector + filter conditions)\n",
        "\n",
        "- Provides a RESTful API and gRPC, with official clients for Python, JS, etc.\n",
        "\n",
        "- Designed to integrate well with AI models, especially LLMs and embeddings"
      ],
      "metadata": {
        "id": "hTFhFzfi5Xzh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üí° When & Why Use Qdrant?\n",
        "Use Qdrant when you need:\n",
        "\n",
        "- Semantic search (search by meaning instead of keywords)\n",
        "\n",
        "- Question answering systems\n",
        "\n",
        "- RAG pipelines (for LLMs like ChatGPT)\n",
        "\n",
        "- Clustering and recommendation based on vector proximity\n",
        "\n",
        "- Handling millions of records efficiently with approximate nearest neighbor (ANN) search"
      ],
      "metadata": {
        "id": "gUPC2L-86Pfg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U langchain langchain-openai langchain-qdrant langchain-community qdrant_client openai tiktoken"
      ],
      "metadata": {
        "id": "E8XVXUGW3-z7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa998ae1-0503-4f82-f217-7e3585b931e8"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import qdrant\n",
        "from langchain_qdrant import QdrantVectorStore\n",
        "# from langchain.embeddings.openai import OpenAIEmbeddings  # deprecated\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from qdrant_client import QdrantClient, models\n",
        "import os\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "-7IIEHZ84HO4"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IMPORTANT NOTE!!!** In order to safely use the api keys and secrets on Google Colab, it is good practice and highly recommended to manually insert them (rather than hardcoding).\n",
        "\n",
        "**Avoid**...\n",
        "```\n",
        "api_key = \"sk-abc123...\"  # Hardcoded   ‚ùå\n",
        "```"
      ],
      "metadata": {
        "id": "--98ENlHVUvJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below you can how to retrieve the secret you have already inserted into your Google Colab acc.:"
      ],
      "metadata": {
        "id": "QHAPBmf0YAGN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set qdrant api key via colab secrets\n",
        "os.environ[\"QDRANT_API_KEY\"] = userdata.get('qdrant-api-key')\n",
        "# Set qdrant host\n",
        "os.environ[\"QDRANT_HOST\"] = \"https://ff95ca73-34db-4c16-8655-f9cd97f62ec9.europe-west3-0.gcp.cloud.qdrant.io:6333\""
      ],
      "metadata": {
        "id": "sj3Md1nJ7lFb"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a qdrant client\n",
        "client = QdrantClient(\n",
        "    url=os.environ[\"QDRANT_HOST\"],\n",
        "    api_key=os.environ[\"QDRANT_API_KEY\"]\n",
        ")"
      ],
      "metadata": {
        "id": "AeaLzNQB9TOG"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create collections\n",
        "\n",
        "os.environ[\"COLLECTION_NAME\"] = \"my_collection\"\n",
        "\n",
        "client.create_collection(\n",
        "    collection_name=os.getenv(\"COLLECTION_NAME\"),\n",
        "    vectors_config=models.VectorParams(size=1536,   # 768 for instructor-x1, 1536 for OpenAI\n",
        "                                      distance=models.Distance.COSINE),\n",
        ")\n",
        "\n",
        "# to delete the collection, uncomment the line below\n",
        "# client.delete_collection(collection_name=os.getenv(\"COLLECTION_NAME\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RzmkNFz0VA8g",
        "outputId": "b86fc6fc-222e-4e19-dfef-4fc38b621ec3"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can check whether the client has been properly established or not simply through **Thunder Client** for instance. Here‚Äôs a very brief step-by-step guide to sending requests using Thunder Client in VS Code:\n",
        "\n",
        "1. Install Thunder Client\n",
        "  - Go to the Extensions tab in VS Code\n",
        "  - Search for Thunder Client and install it\n",
        "\n",
        "2. Open Thunder Client\n",
        "\n",
        "  - Click the Thunder Client icon in the sidebar\n",
        "\n",
        "3. Create a New Request\n",
        "\n",
        "  - Click ‚ÄúNew Request‚Äù\n",
        "  - Choose method: GET, POST, etc.\n",
        "  - Enter the request URL\n",
        "\n",
        "4. Set Headers / Body (if needed)\n",
        "\n",
        "  - Use the Headers tab to add content-type, auth, etc.\n",
        "  - Use the Body tab for POST, PUT, etc. requests (JSON, form-data, etc.)\n",
        "\n",
        "5. Send Request\n",
        "\n",
        "  - Click the ‚ÄúSend‚Äù button\n",
        "  - View response in the bottom panel\n",
        "\n",
        "Done! üöÄ"
      ],
      "metadata": {
        "id": "AlJ1Afd8ZbFf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to create chunks\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "\n",
        "# Define function\n",
        "def create_chunks(text):\n",
        "  # split the text intor chunks\n",
        "  text_splitter = CharacterTextSplitter(\n",
        "      separator=\"\\n\",\n",
        "      chunk_size=500,\n",
        "      chunk_overlap=50,\n",
        "      length_function=len\n",
        "  )\n",
        "  chunks = text_splitter.split_text(text)\n",
        "\n",
        "  return chunks"
      ],
      "metadata": {
        "id": "HZSTlNOQDR8O"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read data and create chunks\n",
        "\n",
        "with open(\"./of-mice-and-men.txt\") as f:\n",
        "  text = f.read()\n",
        "\n",
        "texts = create_chunks(text)"
      ],
      "metadata": {
        "id": "jjGHJ3qFE9SU"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(texts))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4Sq0TugLpQ0",
        "outputId": "47beac3f-a636-46f4-e4ad-6afec5b1aeba"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(texts[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eiZm2b9PLxEW",
        "outputId": "672ae71a-b04f-46cc-9ffc-03775888a3c5"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In \"Of Mice and Men,\" George tells Lennie about their shared dream of owning a small farm where Lennie can care for rabbits. This dream serves as a source of hope and comfort for Lennie, a mentally disabled man, and also as a shared ambition for George and Lennie. The rabbits specifically represent the peaceful and fulfilling life they hope to create together, a stark contrast to the harsh realities of their current working lives.\n",
            "Elaboration:\n",
            "The Dream:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Count how many points (vectors) exist\n",
        "count = client.count(collection_name=\"my_collection\", exact=True)\n",
        "print(f\"Total vectors: {count.count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kz5j2eDTyPBa",
        "outputId": "152317a3-d294-4031-9503-6a828114c963"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total vectors: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get info about a specific collection\n",
        "collection_info = client.get_collection(\"my_collection\")\n",
        "print(collection_info)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6sq6DmNlPhcr",
        "outputId": "068f001f-70c8-4297-9736-e5060aafe523"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "status=<CollectionStatus.GREEN: 'green'> optimizer_status=<OptimizersStatusOneOf.OK: 'ok'> vectors_count=None indexed_vectors_count=0 points_count=0 segments_count=2 config=CollectionConfig(params=CollectionParams(vectors=VectorParams(size=1536, distance=<Distance.COSINE: 'Cosine'>, hnsw_config=None, quantization_config=None, on_disk=None, datatype=None, multivector_config=None), shard_number=1, sharding_method=None, replication_factor=1, write_consistency_factor=1, read_fan_out_factor=None, on_disk_payload=True, sparse_vectors=None), hnsw_config=HnswConfig(m=16, ef_construct=100, full_scan_threshold=10000, max_indexing_threads=0, on_disk=False, payload_m=None), optimizer_config=OptimizersConfig(deleted_threshold=0.2, vacuum_min_vector_number=1000, default_segment_number=0, max_segment_size=None, memmap_threshold=None, indexing_threshold=20000, flush_interval_sec=5, max_optimization_threads=None), wal_config=WalConfig(wal_capacity_mb=32, wal_segments_ahead=0), quantization_config=None, strict_mode_config=StrictModeConfig(enabled=False, max_query_limit=None, max_timeout=None, unindexed_filtering_retrieve=None, unindexed_filtering_update=None, search_max_hnsw_ef=None, search_allow_exact=None, search_max_oversampling=None, upsert_max_batchsize=None, max_collection_vector_size_bytes=None, read_rate_limit=None, write_rate_limit=None, max_collection_payload_size_bytes=None, filter_max_conditions=None, condition_max_size=None)) payload_schema={}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create vector store\n",
        "\n",
        "# Set openai api key via colab secrets\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('openai-api-key')\n",
        "\n",
        "# Initialize the embedding model\n",
        "embeddings = OpenAIEmbeddings()\n",
        "\n",
        "\n",
        "# check if the vectore store has already been loaded\n",
        "# if not load the vector store\n",
        "if count.count == 0:\n",
        "  doc_store = QdrantVectorStore.from_texts(\n",
        "      texts,\n",
        "      embeddings,\n",
        "      url=os.environ[\"QDRANT_HOST\"],\n",
        "      api_key=os.environ[\"QDRANT_API_KEY\"],\n",
        "      collection_name=\"my_collection\"\n",
        "  )\n",
        "  print(\"Vector store created!\")\n",
        "else:\n",
        "  doc_store = QdrantVectorStore.from_existing_collection(\n",
        "      embeddings=embeddings,\n",
        "      collection_name=\"my_collection\",\n",
        "      url=os.environ[\"QDRANT_HOST\"],\n",
        "      api_key=os.environ[\"QDRANT_API_KEY\"],\n",
        "  )\n",
        "  print(\"Vector store loaded!\")"
      ],
      "metadata": {
        "id": "HJQ8vt-HWagw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b2ac188-51c0-4e98-dd4b-f93e7e99983e"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector store created!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Count how many points (vectors) exist\n",
        "count = client.count(collection_name=\"my_collection\", exact=True)\n",
        "print(f\"Total vectors: {count.count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "myebftHkbEyf",
        "outputId": "c697cfdd-d6b9-4599-ab29-ebf82507395c"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total vectors: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement QA chain\n",
        "\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "# Initialize the QA chain\n",
        "qa = RetrievalQA.from_chain_type(\n",
        "    llm=OpenAI(),\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=doc_store.as_retriever()\n",
        ")"
      ],
      "metadata": {
        "id": "TaZ_NOPMcZzC"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a query\n",
        "\n",
        "query = \"What provides George with a motivation to work hard?\"\n",
        "\n",
        "response = qa.run(query)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0FhS91rdk1y",
        "outputId": "327c3646-c39e-4a1c-a817-d66b6432ebd7"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-52-4d6e60f2492b>:5: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  response = qa.run(query)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " The dream of owning a farm and having rabbits provides George with a motivation to work hard and find a place for themselves.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bRbh_OzneHfZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}