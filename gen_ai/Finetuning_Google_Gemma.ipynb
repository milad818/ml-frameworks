{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HrBOogMg5xDF"
   },
   "source": [
    "## Fine-tuning LLMs\n",
    "\n",
    "**LLM fine-tuning** is the process of adapting a pre-trained large language model (LLM) to a specific task or dataset by further training it on a smaller, specialized dataset. Instead of training a model from scratch, which is computationally expensive, fine-tuning leverages the pre-trained model's existing knowledge and fine-tunes it to better suit a particular application.\n",
    "\n",
    "### Here's a more detailed explanation:\n",
    "\n",
    "#### Pre-trained LLMs:\n",
    "LLMs like GPT are trained on massive datasets of text, giving them a broad understanding of language.\n",
    "#### Fine-tuning:\n",
    "Fine-tuning takes this pre-trained model and trains it further using a smaller, more specific dataset related to the task you want the model to perform.\n",
    "#### Task Specialization:\n",
    "This additional training allows the model to learn the nuances of the specific task and domain, making it more accurate and effective for that application.\n",
    "#### Benefits:\n",
    "Fine-tuning offers several advantages, including:\n",
    "\n",
    "**Improved Accuracy:** The model becomes more specialized and accurate for the specific task.\n",
    "\n",
    "**Efficiency:** It's faster and more resource-efficient than training a model from scratch.\n",
    "\n",
    "**Domain Expertise:** The model can learn specialized knowledge within a particular domain.\n",
    "\n",
    "#### Example:\n",
    "Imagine you have a pre-trained model that's good at summarizing text. Fine-tuning it on a dataset of legal documents would make it better at summarizing legal documents, even though it wouldn't have specialized legal knowledge before.\n",
    "In essence, fine-tuning allows you to leverage the power of pre-trained LLMs while tailoring them to your specific needs and achieving higher performance on particular tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vZcJMZH73CBM"
   },
   "source": [
    "## What is LoRA? Why LoRA?\n",
    "\n",
    "**LoRA (Low-Rank Adaptation)** is a technique for fine-tuning large language models (LLMs) by only updating a small number of trainable parameters, rather than all the model's weights. This makes the process more efficient, cost-effective, and memory-friendly compared to traditional fine-tuning methods. LoRA achieves this by decomposing large weight matrices into smaller, low-rank matrices, which are then used to update the original model's parameters.\n",
    "\n",
    "### Here's a more detailed breakdown:\n",
    "\n",
    "#### Parameter-Efficient Fine-Tuning:\n",
    "LoRA is a form of **parameter-efficient fine-tuning (PEFT)**, which aims to reduce the computational cost and memory requirements of fine-tuning large models.\n",
    "\n",
    "#### Low-Rank Decomposition:\n",
    "LoRA identifies that the changes needed for fine-tuning often have a lower \"intrinsic rank\" than the full model's parameters. It leverages this by decomposing large weight matrices into smaller, low-rank matrices.\n",
    "\n",
    "#### Trainable Parameters:\n",
    "LoRA only updates the new, low-rank matrices, keeping the original model's parameters frozen. This significantly reduces the number of trainable parameters.\n",
    "\n",
    "#### Benefits:\n",
    "**Faster Training:** LoRA allows for faster fine-tuning, as it involves updating a smaller set of parameters.\n",
    "\n",
    "**Reduced Memory Usage:** LoRA requires less memory during training and inference, as it only needs to store the low-rank matrices.\n",
    "\n",
    "**Smaller Models:** The resulting LoRA-fine-tuned model can be smaller, making it easier to store and share.\n",
    "\n",
    "#### Applications:\n",
    "LoRA is widely used for fine-tuning LLMs for various tasks, such as **instruction following**, **text summarization**, and **code generation**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dHxb02lXc-vZ"
   },
   "source": [
    "In order to start with coding and taking various steps to fine-tune our model, we have to firstly install the necessary packages and libraries. Below you can see a list of required libraries:\n",
    "\n",
    "- **bitsandbytes** — Library for 8-bit and 4-bit model compression to make models faster and smaller.\n",
    "\n",
    "- **peft** — Parameter-Efficient Fine-Tuning methods like LoRA to fine-tune big models with fewer resources.\n",
    "\n",
    "- **trl** — Transformer Reinforcement Learning tools, mainly for RLHF (Reinforcement Learning with Human Feedback).\n",
    "\n",
    "- **accelerate** — Simplifies multi-GPU, mixed-precision, and distributed training — no need to write boilerplate code.\n",
    "\n",
    "- **datasets** — Huge collection of ready-to-use datasets and easy tools to load, preprocess, and manage data.\n",
    "\n",
    "- **transformers** — Hugging Face’s core library to use, train, and fine-tune transformer models (BERT, GPT, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m104.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m74.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m49.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m87.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.0/411.0 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m336.4/336.4 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m354.7/354.7 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m75.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "# install the necessary packages/libraries\n",
    "!pip3 install -q -U bitsandbytes\n",
    "!pip3 install -q -U peft\n",
    "!pip3 install -q -U trl\n",
    "!pip3 install -q -U accelerate\n",
    "!pip3 install -q -U datasets\n",
    "!pip3 install -q -U transformers\n",
    "# !pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tool/libraries\n",
    "import os\n",
    "import transformers\n",
    "import torch\n",
    "from google.colab import userdata\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import GemmaTokenizer, BitsAndBytesConfig, TrainingArguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jx--dwInExTc"
   },
   "source": [
    "**NOTE!** Remember to SFTConfig together with SFTTrainer; otherwise, it will raise an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set huggingface access token via colab secrets\n",
    "os.environ[\"hf_access_token\"] = userdata.get('hf-access-token')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pP7bw-RvtEfm"
   },
   "source": [
    "**NOTE!** `userdata.get()` works if you previously set the secret using Colab Enterprise features. In normal Colab, this may not exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"google/gemma-2b\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # Load the model weights in 4 bits instead of the usual 16 or 32 bits — this massively saves memory and speed.\n",
    "    # bnb_4bit_use_double_quant=True,  # If used, it would apply a second layer of quantization (making the model even smaller, sometimes at a slight quality cost).\n",
    "    bnb_4bit_quant_type=\"nf4\",  # You can use NF4 (Normalized Float 4) as the quantization scheme.\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16 # During computation, use bfloat16 (brain float 16).\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "77saLxEBCIbu"
   },
   "source": [
    "**NOTE!** Even though Gemma is open-source, you might need to get access or obtain authorization to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e22c9c9c74594dfa94fdcccf1cfe28b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/33.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d992b7095d34ab6a51326cfa5c33ca8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15cd4ed5d77a460d88e475091dbb3969",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7c65ff3a36f4c948d79b7cc4b4e24fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6404df5a3a1646259ee6ce1447499344",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/627 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2509e1b091584a74b86c6be03c1cd420",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/13.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e31941c2e636409299b4376f59c355b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a222def048b84c31b4ee3988839e6a1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b8116fdaf374482a059a00697d1c9eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/67.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed1a46b5da4e44429cdf4db7a2f72b0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe0295370e8a4e6eae807c9006d0735d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# dynamically load the correct tokenizer for a given model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, token=os.environ[\"hf_access_token\"])\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id,\n",
    "                                             quantization_config=bnb_config,\n",
    "                                             device_map={\"\":0},\n",
    "                                             token=os.environ[\"hf_access_token\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qg0ahxIN0-xR"
   },
   "source": [
    "**NOTE!** `device_map=\"auto\"` automatically places the model on CPU, GPU, or multiple devices for optimal performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EGZyXR0W1sQ2"
   },
   "source": [
    "#### AutoModelForSeq2SeqLM vs. AutoModelForCausalLM\n",
    "| **AutoModelForSeq2SeqLM** | **AutoModelForCausalLM** |\n",
    "|:--------------------------|:-------------------------|\n",
    "| For **sequence-to-sequence tasks** (like translation, summarization). | For **causal (left-to-right) language modeling** (like text generation, chatting). |\n",
    "| Input → Encoder → Decoder → Output. (Two parts: encoder + decoder.) | Just one part: **decoder-only** model. Predicts next token based only on previous ones. |\n",
    "| Example models: T5, BART, mBART. | Example models: GPT-2, GPT-3, GPT-NeoX. |\n",
    "| Needs both **input_ids** and **decoder_input_ids** during training. | Only needs **input_ids**. |\n",
    "| Use when you want the model to **transform** an input into an output (input → output). | Use when you want the model to **continue** some text (output only). |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quote: Imagination is more, than knowledge.\n",
      "\n",
      "I am a self-taught artist, born in 1985 in the beautiful city of Porto Alegre, Brazil.\n",
      "\n",
      "I have a degree in Fine Arts from the University of Passo Fundo, in the state of Rio\n"
     ]
    }
   ],
   "source": [
    "text = \"Quote: Imagination is more,\"\n",
    "device = \"cuda:0\"\n",
    "input = tokenizer(text, return_tensors=\"pt\").to(device) # Converts the text into model-friendly tensors (PyTorch format: \"pt\") and moves them to the GPU\n",
    "outputs = model.generate(**input, max_new_tokens=50)  # The model predicts up to 50 new tokens after the given prompt\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True)) # Converts the generated tokens back to human-readable text, skipping special tokens like <EOS> or <PAD>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Os5vRsjzIJqJ"
   },
   "source": [
    "Note that according to the table above, the model is trained for causal (lef-to-right) modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aE3ee6QOJrTR"
   },
   "source": [
    "Let us make a very small change to `text` removing the comma and see the result generating outputs once again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quote: Imagination is more important than knowledge. Knowledge is limited. Imagination encircles the world.\n",
      "\n",
      "- Albert Einstein\n",
      "\n",
      "The following is a list of the most popular and most recent quotes from Albert Einstein.\n",
      "\n",
      "<h2>Albert Einstein Quotes</h2>\n",
      "\n",
      "1. “Imagination is more\n"
     ]
    }
   ],
   "source": [
    "text = \"Quote: Imagination is more\"\n",
    "device = \"cuda:0\"\n",
    "input = tokenizer(text, return_tensors=\"pt\").to(device) # Converts the text into model-friendly tensors (PyTorch format: \"pt\") and moves them to the GPU\n",
    "outputs = model.generate(**input, max_new_tokens=50)  # The model predicts up to 50 new tokens after the given prompt\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True)) # Converts the generated tokens back to human-readable text, skipping special tokens like <EOS> or <PAD>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WwrF4oVoKPry"
   },
   "source": [
    "It is clearly noticeable that a different result has been produced/generated which actually does exist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DO1IcC50MnzD"
   },
   "source": [
    "Below is created a LoRA (Low-Rank Adaptation) configuration to fine-tune a large language model more efficiently.\n",
    "This is done by modifying only a small number of parameters (low-rank matrices) instead of the entire model — which saves memory and speeds up training.\n",
    "\n",
    "The config is built using the LoraConfig class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hwIQB6CtNBN5"
   },
   "source": [
    "But before creating a LoRA configuration, it is necessary to set `os.environ[\"WANDB_DISABLED\"] = \"false\"`. It’s an environment variable used to control whether Weights & Biases (wandb) — a machine learning experiment tracking tool — is enabled or disabled.\n",
    "\n",
    "Setting:\n",
    "\n",
    "- `\"true\"` → Disable wandb logging.\n",
    "\n",
    "- `\"false\"` → Enable wandb logging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vdhswkpaOAJf"
   },
   "source": [
    "#### Why is WANDB enabled before setting the LoRA config?\n",
    "Because when you initialize fine-tuning objects like LoraConfig, training scripts, or trainer classes, they might automatically start logging configuration info (like hyperparameters, model architecture, etc.) to wandb if it's enabled. **Weights & Biases (W&B or WandB)** is the AI developer platform, with tools for training models, fine-tuning models, and leveraging foundation models.\n",
    "\n",
    "If you don't enable wandb early enough, it won't track:\n",
    "\n",
    "- Your LoRA parameters (r, alpha, etc.)\n",
    "- Model settings\n",
    "- Training metrics (loss, accuracy, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_DISABLED\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r = 8,\n",
    "    # lora_alpha = 16,\n",
    "    target_modules = [\"q_proj\", \"0_proj\", \"k_proj\", \"v_proj\", \"gate_proj\",\n",
    "                        \"up_proj\", \"down_proj\"],\n",
    "    # lora_dropout = 0.05,\n",
    "    # bias = \"none\",\n",
    "    task_type = \"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TrX_900iMWFR"
   },
   "source": [
    "The following table represents a breakdown of\n",
    "the LoRA configuration parameters:\n",
    "\n",
    "| Parameter        | Value                          | Meaning |\n",
    "|:-----------------|:-------------------------------|:--------|\n",
    "| `r`              | `8`                            | Rank of the LoRA update matrices (smaller matrices for parameter-efficient tuning). |\n",
    "| `lora_alpha`     | `16`                           | Scaling factor that adjusts the impact of LoRA updates (like a learning rate multiplier). |\n",
    "| `target_modules` | `[\"q_proj\", \"0_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]` | Specific model layers where LoRA is applied (e.g., attention and feedforward layers). |\n",
    "| `lora_dropout`   | `0.05`                         | Dropout rate during training to prevent overfitting on LoRA updates. |\n",
    "| `bias`           | `\"none\"`                       | Whether to apply LoRA to bias terms (here, no bias is fine-tuned). |\n",
    "| `task_type`      | `\"CAUSAL_LM\"`                   | Type of task: **Causal Language Modeling** (predict next token, e.g., for chatbots). |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3580872482d847ea9700655f6feff447",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.55k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd11db9698be49b8918f9f864c9f9db1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "quotes.jsonl:   0%|          | 0.00/647k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1558c6f209e49108013451691c85103",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/2508 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51ee503844644eb1944329ed4e4cdd2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2508 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset(\"Abirate/english_quotes\")\n",
    "data = data.map(lambda samples: tokenizer(samples['quote']), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2508"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of entries in the dataset (english_quotes)\n",
    "len(data[\"train\"][\"quote\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to properly format data instances\n",
    "def format_data(example):\n",
    "  text = f\"Quote: {example['quote'][0]} \\n Author: {example['author'][0]}\"\n",
    "  return [text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['quote', 'author', 'tags', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 2508\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train data info\n",
    "data['train']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/trl/trainer/sft_trainer.py:465: UserWarning: You passed a dataset that is already processed (contains an `input_ids` field) together with a formatting function. Therefore `formatting_func` will be ignored. Either remove the `formatting_func` or pass a dataset that is not already processed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59440febffdb452b97dbbe1c2bec690c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/2508 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "# implement trainer\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    train_dataset = data[\"train\"],\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 1,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 2,\n",
    "        max_steps = 100,\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = True,\n",
    "        logging_steps = 1,\n",
    "        output_dir = \"outputs\",\n",
    "        optim = \"paged_adamw_8bit\"\n",
    "    ),\n",
    "    peft_config = lora_config,\n",
    "    # dataset_text_field = \"text\",\n",
    "    # max_seq_length = 512,\n",
    "    formatting_func=format_data,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4YfbpfoebcUr"
   },
   "source": [
    "Now what? It is time to re-train and actually fine-tune our model..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eTQHHKZlcCFR"
   },
   "source": [
    "As you start to train, it might ask you to enter your W&B API key. Therefore, it is wise to have already created an account so that you can easily have access to a W&B API key on your account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n        window._wandbApiKey = new Promise((resolve, reject) => {\n            function loadScript(url) {\n            return new Promise(function(resolve, reject) {\n                let newScript = document.createElement(\"script\");\n                newScript.onerror = reject;\n                newScript.onload = resolve;\n                document.body.appendChild(newScript);\n                newScript.src = url;\n            });\n            }\n            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n            const iframe = document.createElement('iframe')\n            iframe.style.cssText = \"width:0;height:0;border:none\"\n            document.body.appendChild(iframe)\n            const handshake = new Postmate({\n                container: iframe,\n                url: 'https://wandb.ai/authorize'\n            });\n            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n            handshake.then(function(child) {\n                child.on('authorize', data => {\n                    clearTimeout(timeout)\n                    resolve(data)\n                });\n            });\n            })\n        });\n    ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "wandb: Paste an API key from your profile and hit enter:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ··········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmilad818\u001b[0m (\u001b[33mmilad818-myorg\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20250413_181259-9r30ng7d</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/milad818-myorg/huggingface/runs/9r30ng7d' target=\"_blank\">outputs</a></strong> to <a href='https://wandb.ai/milad818-myorg/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/milad818-myorg/huggingface' target=\"_blank\">https://wandb.ai/milad818-myorg/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/milad818-myorg/huggingface/runs/9r30ng7d' target=\"_blank\">https://wandb.ai/milad818-myorg/huggingface/runs/9r30ng7d</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 02:02, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.560100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.627000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.481400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.753800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.301900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.480500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.886700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.250700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3.195600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.230500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.692800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.742100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.644700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>2.583300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.980900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>2.232400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.606500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>2.522300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.512800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.410500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>2.388800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>2.360000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>1.076100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>2.654500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>2.264200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>1.831700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>1.988600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>2.417200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>1.736000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.454000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>2.863200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>2.204000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>2.620400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>2.199000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.996400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>2.422400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>2.699800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>2.319200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>1.907900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.144800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.918200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>2.741500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.735200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>2.826100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.770300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>2.251800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>1.974200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>1.812900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>1.947100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.488600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>2.512600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>2.043400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>2.978900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>2.187100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>1.757400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>2.237700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>2.097300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>1.527900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>2.082400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.048100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>1.513600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>2.799900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>1.078700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>2.129100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>2.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>1.906900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>2.145500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>1.137900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>1.270000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.347500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.996400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>1.451200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>1.580800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>2.514700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>1.672600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>1.561200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>2.899600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>1.304100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>1.880200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.148600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>2.744800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>2.109900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>1.640700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>2.005900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>2.679400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>2.180500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>2.160200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>1.909600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>2.062800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.854500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>2.283500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>3.125700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>2.664800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>2.388000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>1.536300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>2.713600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>1.885100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>2.545100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>1.308500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.350200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/peft/utils/other.py:1107: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-67fbff2d-1cdfd69f0f1ce82743a4516c;eb67e98a-3e41-417a-bbbc-4eb351cb5092)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/google/gemma-2b/resolve/main/config.json.\n",
      "Access to model google/gemma-2b is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in google/gemma-2b.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in google/gemma-2b - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=100, training_loss=2.06271040558815, metrics={'train_runtime': 1239.7834, 'train_samples_per_second': 0.323, 'train_steps_per_second': 0.081, 'total_flos': 189688154431488.0, 'train_loss': 2.06271040558815})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quote: A woman is like a tea bag; you can't tell how strong she is until you put her in hot water.\n",
      "\n",
      "I'\n"
     ]
    }
   ],
   "source": [
    "text2 = \"Quote: A woman is like a tea bag;\"\n",
    "device = \"cuda:0\"\n",
    "input2 = tokenizer(text2, return_tensors=\"pt\").to(device)\n",
    "outputs2 = model.generate(**input2, max_new_tokens=20)\n",
    "print(tokenizer.decode(outputs2[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wK108byKaqQV"
   },
   "source": [
    "**Please Note!**\n",
    "- It is possible that the model still fails to generate indentical responses after only 100 rounds of training.\n",
    "- It is really sensitive to the very last letters or words you insert in your input. That is, in the example above, it firstly produced a wrong answer only because the last word \"bag\" was mistakenly inserted as \"bah\"."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
