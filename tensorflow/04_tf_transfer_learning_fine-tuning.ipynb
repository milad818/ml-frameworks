{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOz5s71WGQ2R0JD0H0R/KqA"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Fine-tuning Transfer Learning"],"metadata":{"id":"4qaAoOAbnS4P"}},{"cell_type":"markdown","source":["In fine-tuning transfer learning the pre-trained model weights from another model are unfrozen and tweaked during to better suit your own data."],"metadata":{"id":"ppkfq0uXoBLd"}},{"cell_type":"markdown","source":["Fine-tuning is a type of transfer learning where you:\n","\n","Start with a pre-trained model (e.g., ResNet, BERT, GPT).\n","\n","Adjust its parameters (weights) by continuing training on your smaller, task-specific dataset.\n","\n","In this section you can optionally:\n","\n","1. Freeze some layers (e.g., early layers that detect generic features like edges/word syntax).\n","\n","2. Train the entire model end-to-end with a lower learning rate.\n","\n","OR\n","\n","3. Update only the later layers (more task-specific features like organs in X-rays or sentiment in reviews), which we have already implemented in the previous notebook.\n","\n"],"metadata":{"id":"tTXhwYNyB58Y"}},{"cell_type":"markdown","source":["Let us first import TensorFlow and che its version:"],"metadata":{"id":"-kX5Bbx8E8Ix"}},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"00PYu1B0jzhU","executionInfo":{"status":"ok","timestamp":1758202661209,"user_tz":-120,"elapsed":4382,"user":{"displayName":"Milad Zakhireh","userId":"01326276303537213077"}},"outputId":"8be263d6-2ab9-48e6-da39-69f97d9ee06a"},"outputs":[{"output_type":"stream","name":"stdout","text":["TensorFlow version: 2.19.0\n"]}],"source":["import tensorflow as tf\n","\n","print(f\"TensorFlow version: {tf.__version__}\")"]},{"cell_type":"markdown","source":["And then check if we are using either GPU or CPU?"],"metadata":{"id":"ais7wC43FBJS"}},{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DIALu-utEwzc","executionInfo":{"status":"ok","timestamp":1758202661285,"user_tz":-120,"elapsed":74,"user":{"displayName":"Milad Zakhireh","userId":"01326276303537213077"}},"outputId":"dac9d06d-0f18-4e83-b9ef-928a00ce7a6a"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["/bin/bash: line 1: nvidia-smi: command not found\n"]}]},{"cell_type":"markdown","source":["Also, we have to download the module `helper_functions.py module`:"],"metadata":{"id":"xJgModkXGVfp"}},{"cell_type":"code","source":["import os\n","\n","# GitHub raw URL of the module\n","url = \"https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/refs/heads/main/extras/helper_functions.py\"\n","filename = \"helper_functions.py\"\n","\n","# Download only if not already present\n","if not os.path.exists(filename):\n","    print(f\"Downloading {filename} from GitHub...\")\n","    !wget -q -O {filename} {url}\n","    print(f\"Downloaded {filename} to the current directory.\")\n","else:\n","    print(f\"{filename} already exists in the current directory.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-9aCn49FFJKX","executionInfo":{"status":"ok","timestamp":1758202661534,"user_tz":-120,"elapsed":246,"user":{"displayName":"Milad Zakhireh","userId":"01326276303537213077"}},"outputId":"3a202864-9304-4b58-f016-a19cada8db54"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading helper_functions.py from GitHub...\n","Downloaded helper_functions.py to the current directory.\n"]}]},{"cell_type":"code","source":["# Import helper functions we're going to use\n","from helper_functions import create_tensorboard_callback, plot_loss_curves, unzip_data, walk_through_dir"],"metadata":{"id":"1Ex3lc7THrhD","executionInfo":{"status":"ok","timestamp":1758202661819,"user_tz":-120,"elapsed":283,"user":{"displayName":"Milad Zakhireh","userId":"01326276303537213077"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["Now download data as we did in the previous notebook."],"metadata":{"id":"ko2-SpHBHWyf"}},{"cell_type":"code","source":["import zipfile\n","\n","# Download data\n","!wget https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_10_percent.zip\n","\n","# Unzip the file\n","zip_ref = zipfile.ZipFile(\"10_food_classes_10_percent.zip\", \"r\")\n","zip_ref.extractall()\n","zip_ref.close()\n","\n","# OR alternatively\n","\n","# !wget https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_10_percent.zip\n","# unzip_data(\"10_food_classes_10_percent.zip\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xmcd5uWuGSir","executionInfo":{"status":"ok","timestamp":1758202665482,"user_tz":-120,"elapsed":3660,"user":{"displayName":"Milad Zakhireh","userId":"01326276303537213077"}},"outputId":"6a09e891-1a2a-4815-c020-7d3a765a3d25"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["--2025-09-18 13:37:41--  https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_10_percent.zip\n","Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.126.207, 173.194.206.207, 74.125.132.207, ...\n","Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.126.207|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 168546183 (161M) [application/zip]\n","Saving to: ‘10_food_classes_10_percent.zip’\n","\n","10_food_classes_10_ 100%[===================>] 160.74M   203MB/s    in 0.8s    \n","\n","2025-09-18 13:37:42 (203 MB/s) - ‘10_food_classes_10_percent.zip’ saved [168546183/168546183]\n","\n"]}]},{"cell_type":"markdown","source":["Typically the file structure should be like:"],"metadata":{"id":"xv_OouQgRkc1"}},{"cell_type":"markdown","source":["```\n","10_food_classes_10_percent/          <- parent directory\n","├── train                            <- training images\n","│   ├── pizza\n","│   │   │   1647351.jpg\n","│   │   │   1647352.jpg\n","│   │   │   ...\n","│   └── steak\n","│       │   1648001.jpg\n","│       │   1648050.jpg\n","│       │   ...\n","│\n","└── test                             <- testing images\n","    ├── pizza\n","    │   │   1001116.jpg\n","    │   │   1507019.jpg\n","    │   │   ...      \n","    └── steak\n","        │   100274.jpg\n","        │   1653815.jpg\n","        │   ...\n","```"],"metadata":{"id":"wqCm4FWdRTiL"}},{"cell_type":"code","source":["import tensorflow as tf\n","\n","# Create training and test directories\n","train_dir = \"10_food_classes_10_percent/train/\"\n","test_dir = \"10_food_classes_10_percent/test/\"\n","\n","# Create data inputs\n","IMG_SIZE = (224, 224) # define image size\n","train_data_10_percent = tf.keras.preprocessing.image_dataset_from_directory(directory=train_dir,\n","                                                                            image_size=IMG_SIZE,\n","                                                                            label_mode=\"categorical\", # what type are the labels?\n","                                                                            batch_size=32) # batch_size is 32 by default, this is generally a good number\n","test_data_10_percent = tf.keras.preprocessing.image_dataset_from_directory(directory=test_dir,\n","                                                                           image_size=IMG_SIZE,\n","                                                                           label_mode=\"categorical\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w1tjVtpOHf_S","executionInfo":{"status":"ok","timestamp":1758202665821,"user_tz":-120,"elapsed":337,"user":{"displayName":"Milad Zakhireh","userId":"01326276303537213077"}},"outputId":"870e4717-d0db-4a06-c610-a2f105b3ebe7"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 750 files belonging to 10 classes.\n","Found 2500 files belonging to 10 classes.\n"]}]},{"cell_type":"markdown","source":["Calling a TensorFlow utility above (`tf.keras.preprocessing.image_dataset_from_directory`), we simply and quickly build a dataset of images from a folder structure."],"metadata":{"id":"zPTI0rTNXRao"}},{"cell_type":"markdown","source":["If you want to see an example batch of data, you could use the `take()` method."],"metadata":{"id":"OKfdJ_bSYQiz"}},{"cell_type":"code","source":["# See an example batch of data\n","for images, labels in train_data_10_percent.take(1):\n","  print(images, labels)"],"metadata":{"id":"D48t8jTXJqu5","executionInfo":{"status":"ok","timestamp":1758202666223,"user_tz":-120,"elapsed":403,"user":{"displayName":"Milad Zakhireh","userId":"01326276303537213077"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"9aa528da-d457-4bbb-c5e6-3693e29cc5c5"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor(\n","[[[[1.46127548e+02 7.61275482e+01 1.53010197e+01]\n","   [1.59525513e+02 9.15255127e+01 2.85255127e+01]\n","   [1.42290817e+02 7.18622437e+01 1.07346935e+01]\n","   ...\n","   [1.49862198e+02 5.90764580e+01 6.07645750e+00]\n","   [1.45806198e+02 5.48062057e+01 2.60251731e-01]\n","   [1.58872543e+02 6.88725433e+01 8.15818596e+00]]\n","\n","  [[1.48530609e+02 7.91275558e+01 1.53367357e+01]\n","   [1.58903061e+02 9.17653122e+01 2.41887741e+01]\n","   [1.53000000e+02 8.35000000e+01 1.93571415e+01]\n","   ...\n","   [1.46515259e+02 5.55152588e+01 2.51526070e+00]\n","   [1.45637878e+02 5.47042122e+01 2.90869564e-01]\n","   [1.65974701e+02 7.59747009e+01 1.40665283e+01]]\n","\n","  [[1.49030609e+02 7.98775482e+01 1.32397966e+01]\n","   [1.65367355e+02 9.80255127e+01 2.78979588e+01]\n","   [1.56336731e+02 8.54795914e+01 1.77653046e+01]\n","   ...\n","   [1.44474396e+02 5.26427460e+01 1.47441959e+00]\n","   [1.49785843e+02 5.84134064e+01 2.64293575e+00]\n","   [1.67699020e+02 7.51939545e+01 1.30510254e+01]]\n","\n","  ...\n","\n","  [[6.43878174e+01 5.68163452e+01 4.31735535e+01]\n","   [1.00199203e+02 9.08419952e+01 7.79849396e+01]\n","   [7.18875580e+01 6.18875542e+01 5.03161278e+01]\n","   ...\n","   [2.51428528e+02 2.55000000e+02 2.52000000e+02]\n","   [2.51428528e+02 2.55000000e+02 2.52000000e+02]\n","   [2.51428528e+02 2.55000000e+02 2.52000000e+02]]\n","\n","  [[8.52245331e+01 7.21990128e+01 6.31990166e+01]\n","   [7.67754517e+01 6.37754555e+01 5.47754555e+01]\n","   [8.90559082e+01 7.66987686e+01 6.74844818e+01]\n","   ...\n","   [2.51000000e+02 2.55000000e+02 2.54000000e+02]\n","   [2.51000000e+02 2.55000000e+02 2.54000000e+02]\n","   [2.51000000e+02 2.55000000e+02 2.54000000e+02]]\n","\n","  [[9.25664215e+01 7.63929062e+01 6.93929062e+01]\n","   [8.12195282e+01 6.62195282e+01 5.92195320e+01]\n","   [5.33262405e+01 4.09691010e+01 3.27548141e+01]\n","   ...\n","   [2.51000000e+02 2.55000000e+02 2.54000000e+02]\n","   [2.51000000e+02 2.55000000e+02 2.54000000e+02]\n","   [2.51000000e+02 2.55000000e+02 2.54000000e+02]]]\n","\n","\n"," [[[4.41603928e+01 3.67442589e+01 2.12783794e+01]\n","   [4.27413902e+01 3.57413902e+01 1.90271053e+01]\n","   [4.57726402e+01 3.75184937e+01 2.00408173e+01]\n","   ...\n","   [7.00000000e+00 6.00000000e+00 1.24732666e+01]\n","   [6.66357660e+00 5.66357660e+00 1.36635761e+01]\n","   [7.41614628e+00 5.41614628e+00 1.84161472e+01]]\n","\n","  [[2.07337341e+01 9.87659168e+00 6.81409264e+00]\n","   [3.51393509e+01 2.42107773e+01 1.97417088e+01]\n","   [4.80650482e+01 3.56096954e+01 2.77971935e+01]\n","   ...\n","   [7.78026915e+00 6.78026915e+00 1.32535353e+01]\n","   [7.00414371e+00 5.12944221e+00 1.57535467e+01]\n","   [7.04623842e+00 5.04623842e+00 1.73912964e+01]]\n","\n","  [[1.77735996e+01 2.83641744e+00 2.77423620e-01]\n","   [2.00637779e+01 4.84949207e+00 3.19197118e-01]\n","   [3.90838661e+01 2.25063782e+01 1.30793991e+01]\n","   ...\n","   [6.97765255e+00 5.57744074e+00 1.47780762e+01]\n","   [5.27229977e+00 3.27229977e+00 1.42722998e+01]\n","   [5.00000000e+00 3.00000000e+00 1.40000000e+01]]\n","\n","  ...\n","\n","  [[2.49566940e+02 2.36428238e+02 2.04290115e+02]\n","   [2.50987564e+02 2.37785736e+02 2.06300018e+02]\n","   [2.50785736e+02 2.37622162e+02 2.05741043e+02]\n","   ...\n","   [2.32244354e+02 1.94534454e+02 1.12663750e+02]\n","   [2.33513458e+02 1.92773300e+02 1.00445442e+02]\n","   [2.31045258e+02 1.88906555e+02 8.73344727e+01]]\n","\n","  [[2.47575882e+02 2.35575882e+02 1.97382614e+02]\n","   [2.47973190e+02 2.35973190e+02 1.97830307e+02]\n","   [2.51438431e+02 2.39438431e+02 2.01295547e+02]\n","   ...\n","   [2.30345703e+02 1.95598175e+02 1.23282043e+02]\n","   [2.31000000e+02 1.93129456e+02 1.08821320e+02]\n","   [2.31680130e+02 1.91306412e+02 9.83765411e+01]]\n","\n","  [[2.46168686e+02 2.34752563e+02 1.91000900e+02]\n","   [2.43758896e+02 2.32116074e+02 1.89044540e+02]\n","   [2.48696396e+02 2.36780899e+02 1.96054153e+02]\n","   ...\n","   [2.37205063e+02 2.01156570e+02 1.40062378e+02]\n","   [2.36377899e+02 1.98415192e+02 1.24556366e+02]\n","   [2.33158722e+02 1.92826767e+02 1.10409683e+02]]]\n","\n","\n"," [[[1.59357147e+02 1.41357147e+02 1.29357147e+02]\n","   [1.58668365e+02 1.40668365e+02 1.28668365e+02]\n","   [1.58719391e+02 1.40719391e+02 1.28719391e+02]\n","   ...\n","   [1.52928513e+02 1.30928513e+02 1.17928505e+02]\n","   [1.55285583e+02 1.33285583e+02 1.20285583e+02]\n","   [1.33515259e+02 1.11515259e+02 9.85152588e+01]]\n","\n","  [[1.61285721e+02 1.43285721e+02 1.31285721e+02]\n","   [1.60928574e+02 1.42928574e+02 1.30928574e+02]\n","   [1.61928574e+02 1.43928574e+02 1.31928574e+02]\n","   ...\n","   [1.51300995e+02 1.32300995e+02 1.18300995e+02]\n","   [1.48290649e+02 1.29290649e+02 1.15290649e+02]\n","   [1.31112442e+02 1.12112442e+02 9.81124420e+01]]\n","\n","  [[1.62642853e+02 1.44642853e+02 1.32642853e+02]\n","   [1.60714279e+02 1.42714279e+02 1.30714279e+02]\n","   [1.60428574e+02 1.42428574e+02 1.30428574e+02]\n","   ...\n","   [1.52688721e+02 1.33688721e+02 1.19688728e+02]\n","   [1.43428406e+02 1.24428398e+02 1.10428398e+02]\n","   [1.39020996e+02 1.20020996e+02 1.06020996e+02]]\n","\n","  ...\n","\n","  [[2.50505112e+02 2.42505112e+02 2.31505112e+02]\n","   [2.51826553e+02 2.43826553e+02 2.32826553e+02]\n","   [2.51336716e+02 2.43336716e+02 2.32336716e+02]\n","   ...\n","   [2.34408035e+02 2.19979507e+02 2.06887680e+02]\n","   [2.31841751e+02 2.17841751e+02 2.04841751e+02]\n","   [2.31999969e+02 2.17999969e+02 2.06571442e+02]]\n","\n","  [[2.49760208e+02 2.41760208e+02 2.30760208e+02]\n","   [2.49137787e+02 2.41137787e+02 2.30137787e+02]\n","   [2.50841858e+02 2.42841858e+02 2.31841858e+02]\n","   ...\n","   [2.36515213e+02 2.22086685e+02 2.09086685e+02]\n","   [2.37857117e+02 2.23857117e+02 2.12857117e+02]\n","   [2.35142761e+02 2.21142761e+02 2.10142761e+02]]\n","\n","  [[2.43045609e+02 2.35045609e+02 2.24045609e+02]\n","   [2.47479355e+02 2.39479355e+02 2.28479355e+02]\n","   [2.42795547e+02 2.34795547e+02 2.23795547e+02]\n","   ...\n","   [2.00937088e+02 1.86508560e+02 1.75080032e+02]\n","   [1.97284088e+02 1.83284088e+02 1.72284088e+02]\n","   [1.97697403e+02 1.83697403e+02 1.72697403e+02]]]\n","\n","\n"," ...\n","\n","\n"," [[[1.35867348e+01 8.58673477e+00 2.58673477e+00]\n","   [1.10714283e+01 8.07142830e+00 1.07142830e+00]\n","   [1.07857141e+01 8.21428585e+00 3.21428585e+00]\n","   ...\n","   [1.24285278e+01 9.00000000e+00 2.21426392e+00]\n","   [1.20714417e+01 9.07144165e+00 2.07144165e+00]\n","   [1.20000000e+01 1.10000000e+01 6.00000000e+00]]\n","\n","  [[1.30255098e+01 8.02550983e+00 2.02551007e+00]\n","   [1.19336739e+01 8.93367386e+00 1.93367362e+00]\n","   [9.01530647e+00 8.01530647e+00 3.01530600e+00]\n","   ...\n","   [1.30000000e+01 1.00000000e+01 5.00000000e+00]\n","   [1.30663385e+01 1.00663385e+01 3.06633878e+00]\n","   [1.20000000e+01 1.20000000e+01 4.00000000e+00]]\n","\n","  [[1.30000000e+01 8.00000000e+00 2.00000000e+00]\n","   [1.21989794e+01 9.19897938e+00 2.19897985e+00]\n","   [9.57142830e+00 9.00000000e+00 3.78571415e+00]\n","   ...\n","   [1.33571644e+01 1.07857361e+01 5.83164978e+00]\n","   [1.50000000e+01 1.20000000e+01 7.00000000e+00]\n","   [1.40000000e+01 1.10000000e+01 4.00000000e+00]]\n","\n","  ...\n","\n","  [[2.42859316e+00 8.85712147e+00 0.00000000e+00]\n","   [5.05612373e+00 8.10203743e+00 1.53045058e-02]\n","   [4.28567123e+00 6.81120777e+00 0.00000000e+00]\n","   ...\n","   [1.45714722e+01 1.00000000e+01 5.78573608e+00]\n","   [1.50000000e+01 1.00000000e+01 4.00000000e+00]\n","   [1.46428223e+01 9.64282227e+00 3.64282227e+00]]\n","\n","  [[6.00000000e+00 9.00000000e+00 0.00000000e+00]\n","   [6.06632519e+00 9.06632519e+00 6.63253441e-02]\n","   [6.27041864e+00 9.27041817e+00 2.70418555e-01]\n","   ...\n","   [1.57296019e+01 1.11581297e+01 6.94386578e+00]\n","   [1.40663376e+01 9.06633759e+00 3.06633782e+00]\n","   [1.50255175e+01 1.00255175e+01 4.02551746e+00]]\n","\n","  [[5.28564453e+00 6.28564453e+00 6.42822266e-01]\n","   [6.57139397e+00 7.57139397e+00 1.57139397e+00]\n","   [7.00000000e+00 1.00000000e+01 3.00000000e+00]\n","   ...\n","   [1.35050888e+01 8.93361664e+00 4.71935272e+00]\n","   [1.30714417e+01 8.07144165e+00 2.07144165e+00]\n","   [1.53571777e+01 1.03571777e+01 4.35717773e+00]]]\n","\n","\n"," [[[5.88265305e+01 3.65714302e+01 2.04081631e+00]\n","   [5.46683655e+01 3.16428566e+01 2.11734700e+00]\n","   [5.10051003e+01 3.00663261e+01 3.57142878e+00]\n","   ...\n","   [4.57140684e+00 2.57140684e+00 5.99993467e+00]\n","   [3.78572750e+00 1.81124234e+00 2.73469758e+00]\n","   [6.19899225e+00 5.19899225e+00 3.19899225e+00]]\n","\n","  [[7.34540787e+01 4.85714302e+01 1.50000000e+01]\n","   [7.05714264e+01 4.52857132e+01 1.44897957e+01]\n","   [6.80153046e+01 4.38724480e+01 1.82602043e+01]\n","   ...\n","   [2.19895935e+00 1.98959351e-01 3.59687805e+00]\n","   [4.22451782e+00 2.36228490e+00 2.93877745e+00]\n","   [1.09286413e+01 9.92864132e+00 7.12245369e+00]]\n","\n","  [[8.49387741e+01 5.57908173e+01 2.55051003e+01]\n","   [8.39438705e+01 5.45000000e+01 2.62704067e+01]\n","   [8.30459137e+01 5.37346916e+01 2.85459175e+01]\n","   ...\n","   [4.35714245e+00 2.35714245e+00 5.35714245e+00]\n","   [9.21433735e+00 8.21433735e+00 6.07145405e+00]\n","   [1.79235649e+01 1.69235649e+01 1.26429262e+01]]\n","\n","  ...\n","\n","  [[9.21935558e+00 9.21935558e+00 1.12193556e+01]\n","   [8.72953987e+00 8.72953987e+00 1.07295399e+01]\n","   [7.02037716e+00 7.02037716e+00 9.02037716e+00]\n","   ...\n","   [1.57857056e+01 1.17857056e+01 1.12142334e+01]\n","   [1.40000000e+01 1.30000000e+01 9.00000000e+00]\n","   [1.50765409e+01 1.40765409e+01 9.07654095e+00]]\n","\n","  [[5.42855406e+00 5.42855406e+00 7.42855406e+00]\n","   [2.21425056e+00 2.21425056e+00 4.21425056e+00]\n","   [3.28569698e+00 3.28569698e+00 5.28569698e+00]\n","   ...\n","   [1.50000000e+01 1.10000000e+01 1.04285278e+01]\n","   [1.40000000e+01 1.30000000e+01 9.00000000e+00]\n","   [1.50255060e+01 1.40255060e+01 9.02550602e+00]]\n","\n","  [[4.77040958e+00 4.77040958e+00 4.77040958e+00]\n","   [5.23978758e+00 5.23978758e+00 7.23978758e+00]\n","   [3.36734438e+00 3.36734438e+00 5.36734438e+00]\n","   ...\n","   [1.64285889e+01 1.24285889e+01 1.18571167e+01]\n","   [1.48826323e+01 1.38826323e+01 9.88263226e+00]\n","   [1.18979216e+01 1.08979216e+01 5.89792156e+00]]]\n","\n","\n"," [[[1.06275513e+02 9.92755127e+01 7.10459213e+01]\n","   [1.01622452e+02 9.66224518e+01 6.59591827e+01]\n","   [1.10862244e+02 1.06571426e+02 7.30867310e+01]\n","   ...\n","   [9.74692383e+01 7.54692383e+01 6.38264465e+01]\n","   [7.90152969e+01 5.89438553e+01 5.20867386e+01]\n","   [7.90867920e+01 6.00867920e+01 5.60867920e+01]]\n","\n","  [[9.78469391e+01 9.15153046e+01 6.04183655e+01]\n","   [1.05857147e+02 1.01790817e+02 6.68571472e+01]\n","   [1.02387756e+02 9.88163223e+01 6.15306091e+01]\n","   ...\n","   [9.95560989e+01 7.75560989e+01 6.58010635e+01]\n","   [8.31988983e+01 6.31325607e+01 5.61325836e+01]\n","   [7.39387512e+01 5.49387550e+01 5.07958984e+01]]\n","\n","  [[1.13448982e+02 1.09020409e+02 7.33775482e+01]\n","   [1.31770416e+02 1.27770409e+02 9.00714340e+01]\n","   [1.16811218e+02 1.13688774e+02 7.32857132e+01]\n","   ...\n","   [9.15050507e+01 6.95050507e+01 5.62907867e+01]\n","   [1.00423569e+02 8.04235687e+01 7.15664520e+01]\n","   [1.19275734e+02 9.94900208e+01 9.27043076e+01]]\n","\n","  ...\n","\n","  [[7.42755051e+01 5.84897957e+01 3.56428642e+01]\n","   [7.70153198e+01 5.95561600e+01 3.54285965e+01]\n","   [9.58571548e+01 7.56428680e+01 4.82142944e+01]\n","   ...\n","   [2.32566177e+02 1.97071350e+02 1.44229462e+02]\n","   [2.25999908e+02 1.85928436e+02 1.33484573e+02]\n","   [2.18076538e+02 1.70933594e+02 1.22214233e+02]]\n","\n","  [[6.87397003e+01 5.65254707e+01 3.45050659e+01]\n","   [7.58570938e+01 5.99234200e+01 3.69234543e+01]\n","   [9.23978806e+01 7.45713959e+01 4.82703972e+01]\n","   ...\n","   [2.27530426e+02 1.95270248e+02 1.37484390e+02]\n","   [2.19132523e+02 1.79846756e+02 1.23132469e+02]\n","   [2.19020523e+02 1.70903076e+02 1.20112328e+02]]\n","\n","  [[6.58316345e+01 5.67601929e+01 3.56581535e+01]\n","   [6.55204315e+01 5.25255203e+01 2.90000134e+01]\n","   [8.95000000e+01 7.23622437e+01 4.64234657e+01]\n","   ...\n","   [2.17076462e+02 1.85290726e+02 1.25147781e+02]\n","   [2.14443954e+02 1.74086746e+02 1.14035782e+02]\n","   [2.34561523e+02 1.83204315e+02 1.31735001e+02]]]], shape=(32, 224, 224, 3), dtype=float32) tf.Tensor(\n","[[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"," [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n"," [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n"," [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n"," [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"," [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"," [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"," [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n"," [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n"," [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n"," [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]], shape=(32, 10), dtype=float32)\n"]}]},{"cell_type":"markdown","source":["Now we are ready to launch our first model (the baseline)..."],"metadata":{"id":"Ul8x33SGZXLf"}},{"cell_type":"markdown","source":["We’ll use the `tf.keras.applications` module, which provides a collection of pre-trained computer vision models (trained on ImageNet), along with the Keras Functional API to build our model. But what are the right steps one should take...?"],"metadata":{"id":"to-tuJAsZ4RJ"}},{"cell_type":"markdown","source":["Model 0: Building a transfer learning model using the Keras Functional API"],"metadata":{"id":"W3R05j60MtxM"}},{"cell_type":"markdown","source":["In order to create a baseline in our case, you can do as follows:"],"metadata":{"id":"N9o8gLK1E3G3"}},{"cell_type":"markdown","source":["1. Instantiate a pre-trained base model object by choosing a target model such as `EfficientNetV2B0` and setting the `include_top` parameter to `False`.\n","2. Set the base model's `trainable` attribute to `False` to freeze all of the weights in the pre-trained model.\n","3. Define an input layer for our model (data shape expected by the model)\n","4. Normalize the inputs to our model if it requires (e.g., `EfficientNetV2B0`).\n","5. Pass the inputs to the base model.\n","6. Pool the outputs of the base model into a shape compatible with the output activation layer (turn base model output tensors into same shape as label tensors)\n","7. Create an output activation layer via `tf.keras.layers.Dense()`\n","8. Merge the inputs and outputs layer into a new model using `tf.keras.Model()`\n","9. Compile the model, passing the loss function and optimizer\n","10. Fit the model for as many epochs as necessary."],"metadata":{"id":"S7iNiMT-FKcX"}},{"cell_type":"code","source":["# Create base model\n","base_model = tf.keras.applications.EfficientNetV2B0(include_top=False)\n","\n","# Freeze base model layers\n","base_model.trainable = False\n","\n","# Create the input layer\n","inputs = tf.keras.layers.Input(shape=(224, 224, 3), name=\"input_layer\")\n","\n","# If using ResNet50V2, add this to speed up convergence, remove for EfficientNetV2\n","# x = tf.keras.layers.experimental.preprocessing.Rescaling(1./255)(inputs)\n","\n","# Pass the inputs to the base model\n","x = base_model(inputs)\n","\n","print(f\"Shape after base_model: {x.shape}\")\n","\n","# Average pool the outputs of the base model\n","x = tf.keras.layers.GlobalAveragePooling2D(name=\"global_average_pooling_layer\")(x)\n","print(f\"After GlobalAveragePooling2D(): {x.shape}\")\n","\n","# Create the activation layer\n","outputs = tf.keras.layers.Dense(10, activation=\"softmax\", name=\"output_layer\")(x)\n","\n","# Merge inputs and outputs into a new model via Model()\n","model_0 = tf.keras.Model(inputs, outputs)\n","\n","# Compile the model\n","model_0.compile(loss=\"categorical_crossentropy\",\n","                optimizer=tf.keras.optimizers.Adam(),\n","                metrics=[\"accuracy\"])\n","\n","# Fit the model\n","history_0 = model_0.fit(train_data_10_percent,\n","                        epochs=5,\n","                        steps_per_epoch=len(train_data_10_percent),\n","                        validation_data=test_data_10_percent,\n","                        validation_steps=int(0.15 * len(test_data_10_percent)),\n","                        callbacks=[create_tensorboard_callback(dir_name=\"training_logs\",\n","                                                               experiment_name=\"model_0_baseline\")])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HMnK9423C1PE","executionInfo":{"status":"ok","timestamp":1758207223899,"user_tz":-120,"elapsed":387318,"user":{"displayName":"Milad Zakhireh","userId":"01326276303537213077"}},"outputId":"7649ae48-34a9-4b5b-ab46-29c3fb9c356d"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Shape after base_model: (None, 7, 7, 1280)\n","After GlobalAveragePooling2D(): (None, 1280)\n","Saving TensorBoard log files to: training_logs/model_0_baseline/20250918-144718\n","Epoch 1/5\n","\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 3s/step - accuracy: 0.2801 - loss: 2.1429 - val_accuracy: 0.7443 - val_loss: 1.2693\n","Epoch 2/5\n","\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 3s/step - accuracy: 0.7381 - loss: 1.2030 - val_accuracy: 0.7983 - val_loss: 0.8711\n","Epoch 3/5\n","\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 3s/step - accuracy: 0.8286 - loss: 0.8559 - val_accuracy: 0.8494 - val_loss: 0.7135\n","Epoch 4/5\n","\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 3s/step - accuracy: 0.8618 - loss: 0.6578 - val_accuracy: 0.8352 - val_loss: 0.6222\n","Epoch 5/5\n","\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 3s/step - accuracy: 0.8729 - loss: 0.5872 - val_accuracy: 0.8438 - val_loss: 0.5955\n"]}]},{"cell_type":"markdown","source":["What we did exactly is:"],"metadata":{"id":"f9EdlbrbUgwE"}},{"cell_type":"markdown","source":["We took our custom dataset, fed it into a pre-trained model (`EfficientNetV2B0`), let it identify meaningful patterns, and then added our own output layer to match the number of classes we needed.\n","\n","Keras Functional API is used instead of using the Sequential one to create the model."],"metadata":{"id":"D7CfzK2VVbt5"}},{"cell_type":"code","source":[],"metadata":{"id":"wL5ALJBpSQ-X"},"execution_count":null,"outputs":[]}]}