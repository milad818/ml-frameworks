{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80a9fc0d-c3c8-461f-b9d0-e6331c1e376e",
   "metadata": {},
   "source": [
    "## Introduction to ChromaDB and Its Role in Generative AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852a6027-3e66-45ca-b929-9afb0e4818e3",
   "metadata": {},
   "source": [
    "### What is ChromaDB?\n",
    "ChromaDB is an open-source vector database designed for storing and retrieving high dimensional data efficiently. It is particularly useful for applications involving embeddings, such as text, images, and audio. ChromaDB is optimized for semantic search, recommendation systems, and retrieval-augmented generation (RAG) in AI applications.\n",
    "\n",
    "### Why Vector Databases?\n",
    "Traditional databases store structured data like numbers and text, but vector databases store and index numerical representations (embeddings) of unstructured data. These embeddings enable similarity search based on meaning rather than exact matches.\n",
    "\n",
    "### Key Features of ChromaDB\n",
    "- **Fast and Scalable:** Handles large-scale vector search efficiently.\n",
    "- **Built-in Embedding Storage:** Can store embeddings from models like OpenAI, Sentence Transformers, etc.\n",
    "- **Simple API:** Easy to integrate into Python applications.\n",
    "- **Automatic Metadata Handling:** Supports additional filtering and retrieval based on structured metadata.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e41a8c4-f497-4bb8-b2a1-a9a89d9a0688",
   "metadata": {},
   "source": [
    "### How is ChromaDB Related to Generative AI?\n",
    "Generative AI models, such as GPT (Generative Pre-trained Transformer), Stable Diffusion, and DALL·E, generate text, images, or code. However, they sometimes require external knowledge beyond their training data. ChromaDB enhances generative AI in several ways:\n",
    "\n",
    "#### 1. Retrieval-Augmented Generation (RAG)\n",
    "Large Language Models (LLMs) can struggle with factual consistency. RAG integrates retrieved knowledge from vector databases like ChromaDB to improve accuracy.\n",
    "Example:\n",
    "- A user asks about latest medical research.\n",
    "- The system retrieves relevant papers stored as embeddings in ChromaDB.\n",
    "- The LLM generates a response based on retrieved data.\n",
    "#### 2. Semantic Search for AI Applications\n",
    "ChromaDB enables AI models to find similar text/images/audio based on meaning.\n",
    "Example:\n",
    "- Searching for \"renewable energy solutions\" retrieves documents with similar concepts, even if they don’t contain the exact phrase.\n",
    "#### 3. Personalized AI Assistants\n",
    "ChromaDB allows AI assistants to remember past interactions and retrieve relevant data.\n",
    "Example:\n",
    "- A chatbot storing customer queries as embeddings can retrieve related past questions, improving responses.\n",
    "#### 4. Content Recommendation Systems\n",
    "AI models can use ChromaDB for real-time personalized recommendations.\n",
    "Example:\n",
    "- A music recommendation AI stores song embeddings and suggests similar songs based on past user preferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a538b21-8a0f-4a90-815c-c6097508d2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# firstly install the required packages\n",
    "# !pip install chromadb\n",
    "# !pip install openai\n",
    "# !pip install langchain\n",
    "# !pip install tiktoken\n",
    "# !pip install langchain-community\n",
    "# !pip install langchain-openai\n",
    "# !pip install langchain-chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fc9aa26-eae5-4cd1-84d4-5763ed9d2827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the list of packages installed\n",
    "# !pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffc8e740-bc53-47e8-95a3-7ea10fbd5664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: chromadb\n",
      "Version: 0.6.3\n",
      "Summary: Chroma.\n",
      "Home-page: https://github.com/chroma-core/chroma\n",
      "Author: \n",
      "Author-email: Jeff Huber <jeff@trychroma.com>, Anton Troynikov <anton@trychroma.com>\n",
      "License: \n",
      "Location: C:\\Users\\ASUSK5~1\\miniconda3\\Lib\\site-packages\n",
      "Requires: bcrypt, build, chroma-hnswlib, fastapi, grpcio, httpx, importlib-resources, kubernetes, mmh3, numpy, onnxruntime, opentelemetry-api, opentelemetry-exporter-otlp-proto-grpc, opentelemetry-instrumentation-fastapi, opentelemetry-sdk, orjson, overrides, posthog, pydantic, pypika, PyYAML, rich, tenacity, tokenizers, tqdm, typer, typing_extensions, uvicorn\n",
      "Required-by: langchain-chroma\n"
     ]
    }
   ],
   "source": [
    "!pip show chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8bff899-b618-4bc7-a549-4ebe50bde5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to be able to carry with this project, the zip file below is needed\n",
    "# Hence, you can downlaod and unzip it either through the link or the commands below\n",
    "# !wget -q https://www.dropbox.com/s/vs6ocyvpzzncvwh/new_articles.zip\n",
    "# !unzip -q new_articles.zip -d new_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b3603d5-6b10-44c8-8177-a3f667f06b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: wget\n",
      "Version: 3.2\n",
      "Summary: pure python download utility\n",
      "Home-page: http://bitbucket.org/techtonik/python-wget/\n",
      "Author: anatoly techtonik <techtonik@gmail.com>\n",
      "Author-email: \n",
      "License: Public Domain\n",
      "Location: C:\\Users\\ASUSK5~1\\miniconda3\\Lib\\site-packages\n",
      "Requires: \n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8584ab4-d08b-4dec-9e82-131d334c6d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set OpenAI KEY\n",
    "# As you may already know, through the command below the key will be set as an environment variable\n",
    "# and you will no longer need to pass it directly whenever needed.\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"ADD YOUR OPENAI API KEY HERE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c23f2d9-4b5f-49f1-86c9-771cfd64baab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma # deprecated\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings # deprecated\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain import embeddings\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e29af1-8817-491d-bfca-f9a0949c47fa",
   "metadata": {},
   "source": [
    "#### First step is to load our data so that we can manipulate it i.e., split, embed, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f38821e1-df28-4ff2-80cb-65e993601fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from all the text files in new_articles directory\n",
    "loader = DirectoryLoader(\"./articles\", glob=\"./*.txt\", loader_cls=TextLoader, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "228e83e3-b8ef-4bfa-9b95-66abc78a17e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 21/21 [00:00<00:00, 1073.04it/s]\n"
     ]
    }
   ],
   "source": [
    "documents = loader.load()\n",
    "# documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "676118f5-906b-4619-80b0-182825d6e4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a splitter and split data into chunks \n",
    "txt_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "270a43bc-8b00-49dc-8256-2df8fa9e81ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_chunks = txt_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "795feb74-4522-43d9-b739-4efb9710cfe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Signaling that investments in the supply chain sector remain robust, Pando, a startup developing fulfillment management technologies, today announced that it raised $30 million in a Series B round, bringing its total raised to $45 million.\\n\\nIron Pillar and Uncorrelated Ventures led the round, with participation from existing investors Nexus Venture Partners, Chiratae Ventures and Next47. CEO and founder Nitin Jayakrishnan says that the new capital will be put toward expanding Pandoâ€™s global sales, marketing and delivery capabilities.\\n\\nâ€œWe will not expand into new industries or adjacent product areas,â€\\u200c he told TechCrunch in an email interview. â€œGreat talent is the foundation of the business â€” we will continue to augment our teams at all levels of the organization. Pando is also open to exploring strategic partnerships and acquisitions with this round of funding.â€\\u200c'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_chunks[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26efb824-fe47-4514-b820-82ddcd0d3c46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "228"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(txt_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86db5d34-4e74-4b3b-b295-82743973256d",
   "metadata": {},
   "source": [
    "#### Now that the data is loaded and prepared in chunks, everything is set to create the database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595506b5-daa9-448e-9804-dd9200e45b86",
   "metadata": {},
   "source": [
    "**Initialize a Vector Database From Documents/Chunks**\n",
    "\n",
    "When creating the Chroma vector store, provide the persist_directory parameter to ensure data is stored on disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ca45fdb2-709b-46d7-9115-e50b364f4944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database already exists!\n"
     ]
    }
   ],
   "source": [
    "# persist_directory\n",
    "persist_directory = \"./chroma_db\"\n",
    "\n",
    "# Create the embedding object\n",
    "embedding = OpenAIEmbeddings()\n",
    "\n",
    "if os.path.isdir(persist_directory):\n",
    "    print(\"Database already exists!\")\n",
    "else:\n",
    "    print(\"Directory does not exist. Creating the database...\")\n",
    "    chroma_vdb = Chroma.from_documents(documents=txt_chunks,\n",
    "                                  embedding=embedding,\n",
    "                                  persist_directory=persist_directory)\n",
    "    print(\"Database created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccaa358a-2e74-44d6-aabe-154f6ba252b4",
   "metadata": {},
   "source": [
    "`Chroma.from_documents()` creates a new Chroma vector store from the given txt (list of documents). It automatically generates embeddings for all the documents using `embedding_function`.\n",
    "If `persist_directory` is provided, the data is stored on disk for later use. Right after the execution of the code snippet above, a directory name `chroma_db` will be created at the root path where this notebook is stored."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e814f4-bdd1-4e2c-b909-a5e4af1e1576",
   "metadata": {},
   "source": [
    "**NOTE!** By specifying the `persist_directory`, Chroma will automatically handle data persistence, and there's no need to call a separate `persist()` method as in `chroma_vdb.persist()`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e69063a-bf0f-4689-9304-7479d326a56b",
   "metadata": {},
   "source": [
    "**Initialize a Persisted Vector Store**\n",
    "\n",
    "To load the persisted vector store in future sessions, you can initialize the Chroma instance with the same persist_directory as implemented below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "61ec9afc-82df-4723-8bba-86cb9065c7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the persisted vector store\n",
    "vector_store = Chroma(embedding_function=embedding,\n",
    "                          persist_directory=persist_directory,  # Where to save data locally, remove if not necessary\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62841b3e-9537-40b6-ae1b-4f80600c9b3d",
   "metadata": {},
   "source": [
    "`Chroma()` loads an existing Chroma database from persist_directory (if it exists). No new embeddings are created at this step—it simply retrieves the previously stored vectors. If the directory is empty, the Chroma store will be initialized but without any data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e09b4e3-b5b1-4f99-98dc-cb2235b2bc4f",
   "metadata": {},
   "source": [
    "In a nutshell, putting all together, we will have:\n",
    "\n",
    "| Method                      | Creates New Embeddings? | Requires Existing Chroma DB? | Use Case                                      |\n",
    "|-----------------------------|------------------------|-----------------------------|-----------------------------------------------|\n",
    "| `Chroma.from_documents()`   | ✅ Yes                 | ❌ No                        | Creating a new vector store from documents   |\n",
    "| `Chroma()`                  | ❌ No                  | ✅ Yes (or starts empty)     | Loading an existing vector store from disk   |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b5514e88-0bb3-4e0f-ab3e-370cc4f37819",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_chroma.vectorstores.Chroma"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(vector_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ecfe63-aa38-4536-af14-9c1980016bda",
   "metadata": {},
   "source": [
    "#### What next... ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929d7569-5a3b-42e6-8891-272b74bb7381",
   "metadata": {},
   "source": [
    "Great! Now that you've stored your text chunks in a Chroma vector store, the next step is to retrieve relevant chunks based on a user query. This is where a retriever comes in. `vector_store.as_retriever()` will return a retriever that can be used to find documents based on similarity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "19e170b2-c94d-4e05-b753-3f16738bb4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c78a77-7fd9-4390-922b-309ac5398c5b",
   "metadata": {},
   "source": [
    "You can also pass the argument `search_kwargs={\"k\": <NUMBER>}` to `vector_store.as_retriever()` to retrieve top `NUMBER` results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e3aa56-58fd-4d82-8b8a-473f973a49f0",
   "metadata": {},
   "source": [
    "Once the retriever is created, you can query it like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5e291cce-4ee5-43ad-b8ff-8017cb46a2f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='21bd54d0-38a6-401b-8cdc-6a9ba5c96482', metadata={'source': 'articles\\\\05-04-hugging-face-and-servicenow-release-a-free-code-generating-model.txt'}, page_content='â€œOne thing we learned from releases such as Stable Diffusion last year is the creativity and capability of the open-source community,â€\\u200c von Werra told TechCrunch in an email interview. â€œWithin weeks of the release the community had built dozens of variants of the model as well as custom applications. Releasing a powerful code generation model allows anybody to fine-tune and adapt it to their own use-cases and will enable countless downstream applications.â€\\u200c\\n\\nBuilding a model\\n\\nStarCoder is a part of Hugging Faceâ€™s and ServiceNowâ€™s over-600-person BigCode project, launched late last year, which aims to develop â€œstate-of-the-artâ€\\u200c AI systems for code in an â€œopen and responsibleâ€\\u200c way. Hugging Face supplied an in-house compute cluster of 512 Nvidia V100 GPUs to train the StarCoder model.'),\n",
       " Document(id='177b0e0a-3be0-4e5f-97b7-f0f8db0d9fec', metadata={'source': 'articles\\\\05-04-hugging-face-and-servicenow-release-a-free-code-generating-model.txt'}, page_content='Unlike Copilot, the 15-billion-parameter StarCoder was trained over the course of several days on an open source dataset called The Stack, which has over 19 million curated, permissively licensed repositories and more than six terabytes of code in over 350 programming languages. In machine learning, parameters are the parts of an AI system learned from historical training data and essentially define the skill of the system on a problem, such as generating code.\\n\\nBecause itâ€™s permissively licensed, code from The Stack can be copied, modified and redistributed. But the BigCode project also provides a way for developers to â€œopt outâ€\\u200c of The Stack, similar to efforts elsewhere to let artists remove their work from text-to-image AI training datasets.'),\n",
       " Document(id='b5ed394e-83f6-4dfc-abc4-0f629b85b0d0', metadata={'source': 'articles\\\\05-04-hugging-face-and-servicenow-release-a-free-code-generating-model.txt'}, page_content='â€œAt launch, StarCoder will not ship as many features as GitHub Copilot, but with its open-source nature, the community can help improve it along the way as well as integrate custom models,â€\\u200c he said.\\n\\nThe StarCoder code repositories, model training framework, dataset-filtering methods, code evaluation suite and research analysis notebooks are available on GitHub as of this week. The BigCode project will maintain them going forward as the groups look to develop more capable code-generating models, fueled by input from the community.\\n\\nThereâ€™s certainly work to be done. In the technical paper accompanying StarCoderâ€™s release, Hugging Face and ServiceNow say that the model may produce inaccurate, offensive, and misleading content as well as PII and malicious code that managed to make it past the dataset filtering stage.'),\n",
       " Document(id='aa17b232-b4dc-4908-8a14-37c97a0483e4', metadata={'source': 'articles\\\\05-04-hugging-face-and-servicenow-release-a-free-code-generating-model.txt'}, page_content='AI startup Hugging Face and ServiceNow Research, ServiceNowâ€™s R&D division, have released StarCoder, a free alternative to code-generating AI systems along the lines of GitHubâ€™s Copilot.\\n\\nCode-generating systems like DeepMindâ€™s AlphaCode; Amazonâ€™s CodeWhisperer; and OpenAIâ€™s Codex, which powers Copilot, provide a tantalizing glimpse at whatâ€™s possible with AI within the realm of computer programming. Assuming the ethical, technical and legal issues are someday ironed out (and AI-powered coding tools donâ€™t cause more bugs and security exploits than they solve), they could cut development costs substantially while allowing coders to focus on more creative tasks.')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is StarCoder?\"\n",
    "retrieved_docs = retriever.invoke(query)\n",
    "retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "635d4c8c-9687-448d-9702-219a1d447b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â€œOne thing we learned from releases such as Stable Diffusion last year is the creativity and capability of the open-source community,â€‌ von Werra told TechCrunch in an email interview. â€œWithin weeks of the release the community had built dozens of variants of the model as well as custom applications. Releasing a powerful code generation model allows anybody to fine-tune and adapt it to their own use-cases and will enable countless downstream applications.â€‌\n",
      "\n",
      "Building a model\n",
      "\n",
      "StarCoder is a part of Hugging Faceâ€™s and ServiceNowâ€™s over-600-person BigCode project, launched late last year, which aims to develop â€œstate-of-the-artâ€‌ AI systems for code in an â€œopen and responsibleâ€‌ way. Hugging Face supplied an in-house compute cluster of 512 Nvidia V100 GPUs to train the StarCoder model.\n"
     ]
    }
   ],
   "source": [
    "print(retrieved_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b627e4-1353-489a-9a8b-b0e285198c78",
   "metadata": {},
   "source": [
    "**You have a retriever now!** Therefore, you can pass the retrieved documents to a language model (e.g., OpenAI GPT) to generate **refined** answers using the retrieved information based on similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a6bee912-1d75-4072-b04e-7bca982293e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\")\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=llm,\n",
    "                                       chain_type=\"stuff\",\n",
    "                                       retriever=retriever,\n",
    "                                       return_source_documents=True)\n",
    "\n",
    "response = qa_chain.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9d85e3b4-d535-4558-a541-f421ec8ab792",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'What is StarCoder?',\n",
       " 'result': \"StarCoder is a free alternative to code-generating AI systems like GitHub's Copilot. It is a part of the BigCode project and was developed by Hugging Face and ServiceNow Research. StarCoder is a 15-billion-parameter model trained on an open source dataset called The Stack, which contains over 19 million curated, permissively licensed repositories and more than six terabytes of code in over 350 programming languages. StarCoder allows fine-tuning and adaptation by the community and aims to develop state-of-the-art AI systems for code in an open and responsible way.\",\n",
       " 'source_documents': [Document(id='21bd54d0-38a6-401b-8cdc-6a9ba5c96482', metadata={'source': 'articles\\\\05-04-hugging-face-and-servicenow-release-a-free-code-generating-model.txt'}, page_content='â€œOne thing we learned from releases such as Stable Diffusion last year is the creativity and capability of the open-source community,â€\\u200c von Werra told TechCrunch in an email interview. â€œWithin weeks of the release the community had built dozens of variants of the model as well as custom applications. Releasing a powerful code generation model allows anybody to fine-tune and adapt it to their own use-cases and will enable countless downstream applications.â€\\u200c\\n\\nBuilding a model\\n\\nStarCoder is a part of Hugging Faceâ€™s and ServiceNowâ€™s over-600-person BigCode project, launched late last year, which aims to develop â€œstate-of-the-artâ€\\u200c AI systems for code in an â€œopen and responsibleâ€\\u200c way. Hugging Face supplied an in-house compute cluster of 512 Nvidia V100 GPUs to train the StarCoder model.'),\n",
       "  Document(id='177b0e0a-3be0-4e5f-97b7-f0f8db0d9fec', metadata={'source': 'articles\\\\05-04-hugging-face-and-servicenow-release-a-free-code-generating-model.txt'}, page_content='Unlike Copilot, the 15-billion-parameter StarCoder was trained over the course of several days on an open source dataset called The Stack, which has over 19 million curated, permissively licensed repositories and more than six terabytes of code in over 350 programming languages. In machine learning, parameters are the parts of an AI system learned from historical training data and essentially define the skill of the system on a problem, such as generating code.\\n\\nBecause itâ€™s permissively licensed, code from The Stack can be copied, modified and redistributed. But the BigCode project also provides a way for developers to â€œopt outâ€\\u200c of The Stack, similar to efforts elsewhere to let artists remove their work from text-to-image AI training datasets.'),\n",
       "  Document(id='b5ed394e-83f6-4dfc-abc4-0f629b85b0d0', metadata={'source': 'articles\\\\05-04-hugging-face-and-servicenow-release-a-free-code-generating-model.txt'}, page_content='â€œAt launch, StarCoder will not ship as many features as GitHub Copilot, but with its open-source nature, the community can help improve it along the way as well as integrate custom models,â€\\u200c he said.\\n\\nThe StarCoder code repositories, model training framework, dataset-filtering methods, code evaluation suite and research analysis notebooks are available on GitHub as of this week. The BigCode project will maintain them going forward as the groups look to develop more capable code-generating models, fueled by input from the community.\\n\\nThereâ€™s certainly work to be done. In the technical paper accompanying StarCoderâ€™s release, Hugging Face and ServiceNow say that the model may produce inaccurate, offensive, and misleading content as well as PII and malicious code that managed to make it past the dataset filtering stage.'),\n",
       "  Document(id='aa17b232-b4dc-4908-8a14-37c97a0483e4', metadata={'source': 'articles\\\\05-04-hugging-face-and-servicenow-release-a-free-code-generating-model.txt'}, page_content='AI startup Hugging Face and ServiceNow Research, ServiceNowâ€™s R&D division, have released StarCoder, a free alternative to code-generating AI systems along the lines of GitHubâ€™s Copilot.\\n\\nCode-generating systems like DeepMindâ€™s AlphaCode; Amazonâ€™s CodeWhisperer; and OpenAIâ€™s Codex, which powers Copilot, provide a tantalizing glimpse at whatâ€™s possible with AI within the realm of computer programming. Assuming the ethical, technical and legal issues are someday ironed out (and AI-powered coding tools donâ€™t cause more bugs and security exploits than they solve), they could cut development costs substantially while allowing coders to focus on more creative tasks.')]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde7255e-33d3-441a-b232-32c91191466f",
   "metadata": {},
   "source": [
    "To add a bit of flavour to the work, let's define a function which enables us to extract the source documents from the retrieved response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dc560a84-ce6d-4d76-b39a-3626a2517172",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_source(response):\n",
    "    print(\"Resources:\")\n",
    "    for source in response[\"source_documents\"]:\n",
    "        print(source.metadata[\"source\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cca5873a-ca6c-494e-b0e3-bc3084aa61e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resources:\n",
      "articles\\05-04-hugging-face-and-servicenow-release-a-free-code-generating-model.txt\n",
      "articles\\05-04-hugging-face-and-servicenow-release-a-free-code-generating-model.txt\n",
      "articles\\05-04-hugging-face-and-servicenow-release-a-free-code-generating-model.txt\n",
      "articles\\05-04-hugging-face-and-servicenow-release-a-free-code-generating-model.txt\n"
     ]
    }
   ],
   "source": [
    "extract_source(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e406db-2b17-4815-9992-2952cffcbd55",
   "metadata": {},
   "source": [
    "### Last Words\n",
    "Below are some resource documentations where you can explore and find out more about different tools:\n",
    "\n",
    "[- ChromaDB](https://docs.trychroma.com/docs/overview/introduction) \\\n",
    "[- LangChain](https://python.langchain.com/docs/introduction/) \\\n",
    "[- ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html) \\\n",
    "[- Retrieval QA](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.retrieval_qa.base.RetrievalQA.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ec67ba-9b10-43dd-8ffc-70cdd7e1fd23",
   "metadata": {},
   "source": [
    "Good Luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0f1177-16cc-43a3-9484-d973d6ba4b85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
