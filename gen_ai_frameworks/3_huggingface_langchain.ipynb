{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06cb5c17-55c0-4472-8fd7-5cff18bedb00",
   "metadata": {},
   "source": [
    "## Hugging Face\n",
    "#### What is Hugging Face?\n",
    "Hugging Face is a leading open-source platform and community focused on natural language processing (NLP) and machine learning (ML). It provides tools, libraries, and models for tasks such as text generation, translation, classification, summarization, and beyond. Hugging Face has revolutionized accessibility to pre-trained models and democratized AI through its ecosystem of resources. Key Components of Hugging Face are:\n",
    "\n",
    "- **Transformers Library:** A widely used Python library for working with pre-trained transformer models (like BERT, GPT, T5, etc.).\n",
    "- **Datasets Library:** Simplifies loading, preprocessing, and using datasets for training models.\n",
    "- **Model Hub:** A repository of thousands of pre-trained models contributed by the community and organizations, covering diverse tasks and languages.\n",
    "- **Hugging Face API:** A cloud-based API that allows users to access pre-trained models without requiring local infrastructure.\n",
    "- **Spaces:** A platform for hosting and sharing AI models and applications using frameworks like Gradio or Streamlit.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27688ad-bf80-4e22-9b9a-e697d0181ec3",
   "metadata": {},
   "source": [
    "\n",
    "#### How does Hugging Face differ from OpenAI?\n",
    "\n",
    "| **Aspect**                  | **Hugging Face**                                                                                                     | **OpenAI**                                                                                           |\n",
    "|-----------------------------|---------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------|\n",
    "| **Openness**                | Open-source platform with publicly available models and code.                                                       | Primarily closed-source. OpenAI API provides access to its proprietary models like GPT.             |\n",
    "| **Model Availability**      | Hosts a variety of pre-trained LLMs (GPT, BERT, T5, etc.) from multiple organizations and contributors.              | Only provides proprietary models (e.g., GPT-3, GPT-4, Codex, DALL·E).                               |\n",
    "| **Customization**           | Allows **fine-tuning** of models and training custom models locally or in the cloud.                                | Fine-tuning OpenAI models is possible but limited, and requires API access.                        |\n",
    "| **API Flexibility**         | API supports a wide range of models/tasks from the Model Hub (e.g., BERT, RoBERTa, GPT-based models).               | API only gives access to OpenAI’s proprietary models like GPT.                                      |\n",
    "| **Pricing Model**           | Many tools and models are free/open-source. Hugging Face API offers paid tiers for hosted model inference.          | OpenAI is predominantly API-driven and has a consumption-based pricing model for access to its models. |\n",
    "| **Community Contribution**  | Strong focus on community collaboration, with open contributions to the Model Hub and libraries.                   | No community contributions to OpenAI’s proprietary models.                                          |\n",
    "| **Task Diversity**          | Hugging Face supports **transformers** for various domains (text, vision, audio, etc.).                            | OpenAI models are versatile but typically limited to text (and some image) tasks.                   |\n",
    "| **Training Flexibility**    | Users can train their own models from scratch or fine-tune existing models.                                         | OpenAI does not offer training from scratch. Users only interact with pre-trained models via API.   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a84af01-9829-42ae-8c65-51dae2baa397",
   "metadata": {},
   "source": [
    "Hugging Face is closely tied to large language models (LLMs) in the following ways: repository for pre-trained LLMs, fine-tuning LLMs, democratizing LLMs (making them more accessible to developers), and supporting customization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2adeca2d-c58c-4b3b-ba8c-910fdac26055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install huggingface_hub\n",
    "# !pip install transformers\n",
    "# !pip install accelerate\n",
    "# !pip install bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c9d8a3b-b72b-4d24-9ac4-b9cfddf23550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# below import the necessary libraries\n",
    "from langchain import PromptTemplate, HuggingFaceHub, LLMChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c98480-d42f-4231-904c-c5dcef2690ef",
   "metadata": {},
   "source": [
    "#### Before we start...\n",
    "You should access the Hugging Face API token\n",
    "`os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]` is used to access the Hugging Face API token stored in the environment variables.\n",
    "#### When is it used?\n",
    "- When using Hugging Face models via transformers or diffusers\n",
    "- When accessing private models or datasets via huggingface_hub\n",
    "- When fine-tuning or pushing models to Hugging Face Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7d7049e0-62f6-4063-b85d-016f2963bede",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# set the environment\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"Add your own token here!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab58ad14-b4fb-441c-8c06-a1cbd52cbe31",
   "metadata": {},
   "source": [
    "In order to check whether the environment variable has been properly set or not, you can print `os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d579e297-51a9-46e8-898a-c56a8f98be10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a prompt\n",
    "prompt_container = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template= \"What container is suitable for {product}?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2143247-307f-48b8-ba94-f0826f1ef839",
   "metadata": {},
   "source": [
    "We are going to firstly do the warm up with the model `facebook/mbart-large-50` or `google/flan-t5-large`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "14669f7c-5ce7-48f4-b659-7095e10a9c07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuggingFaceHub(client=<InferenceClient(model='facebook/mbart-large-50', timeout=None)>, repo_id='facebook/mbart-large-50', task='text2text-generation', model_kwargs={'temperature': 1.5})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HuggingFaceHub(repo_id=\"facebook/mbart-large-50\", model_kwargs={\"temperature\":1.5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b7ef75eb-20b8-41fa-bd55-4a239336de05",
   "metadata": {},
   "outputs": [],
   "source": [
    "client1 = HuggingFaceHub(repo_id=\"google/flan-t5-large\", model_kwargs={\"temperature\":0})\n",
    "client2 = HuggingFaceHub(repo_id=\"facebook/mbart-large-50\", model_kwargs={\"temperature\":1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e7f59478-6aba-4112-a3c1-6e3ba45cd0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "request_container1 = LLMChain(llm=client, prompt=prompt_container)\n",
    "request_container2 = LLMChain(llm=client, prompt=prompt_container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fd9bb7de-df7a-4736-9aaf-b596417ffe41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tin'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "request_container1.run(\"cake\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99855a90-577f-4d03-a92b-33ae2a4d0b05",
   "metadata": {},
   "source": [
    "**IMPORTANT NOTE!** There is the possibility of having an ERROR raised as the one below: `HFHUBHTTPERROR: 401 client error: unauthorized for url: https://api inference.huggingface.co/models/google/flan-t5-large (request id: orynyb)`\n",
    "In order to avoid such an error make sure the role is **write**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4f90af63-e6b4-4aac-b8fe-30940eb03b54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tin'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "request_container2.run(\"cake\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac9be63-e2f1-4cf6-8163-bdaa7017bdaa",
   "metadata": {},
   "source": [
    "#### In some cases, you might want to utilize the same model locally. \n",
    "\n",
    "Therefore, let me introduce `AutoTokenizer` library.\n",
    "\n",
    "`AutoTokenizer` is a part of the Hugging Face Transformers library. It is an automatic tokenizer class that can dynamically load the correct tokenizer for a given model. Instead of manually specifying the tokenizer type (e.g., `BertTokenizer`, `GPT2Tokenizer`), `AutoTokenizer` automatically selects the appropriate one based on the model name or ID.\n",
    "\n",
    "`AutoTokenizer.from_pretrained(model_id)` This method loads a pre-trained tokenizer from the Hugging Face model hub or from a local directory. The tokenizer is responsible for converting text into tokens and token IDs that the model can understand. is downloaded from the Hugging Face model hub if not already cached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d45317c7-551b-4b9e-9687-0ad666ace995",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "\n",
    "model_id = \"google/flan-t5-large\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9ae65b24-d902-4025-9879-bb131f3de37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_outfit = PromptTemplate(\n",
    "    input_variables=[\"outfit\"],\n",
    "    template=\"What part of the body do(es) (a/an) {outfit} cover?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f0369cde-aea8-41ad-a8da-d79a180dbd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer - the one used particularly to train the model flan-t5-large\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847490e2-e7d6-427f-830f-a6c6ed155f2f",
   "metadata": {},
   "source": [
    "`AutoModelForSeq2SeqLM.from_pretrained(model_id, device_map=\"auto\")` loads a pretrained sequence-to-sequence model (e.g., T5, BART). `device_map=\"auto\"` automatically places the model on CPU, GPU, or multiple devices for optimal performance. Then, the model can process tokenized input and generate text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7f746d5b-1148-46e1-a5fc-10558fe3c6b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu and disk.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_id, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ae86bc08-1d78-4dab-b791-97d74d0eb62d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "pipeline = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "35257271-f09c-467f-824c-ae8d23466696",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_llm = HuggingFacePipeline(pipeline=pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "741b4e02-5e5f-4f32-b947-2814b252074d",
   "metadata": {},
   "outputs": [],
   "source": [
    "request_outfit = LLMChain(llm=local_llm, prompt=prompt_outfit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b1b4111e-d2b9-4a50-8b4f-97e59961dd9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'foot'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "request_outfit.run(\"sock\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "45388ff4-8661-4635-9ba0-e3dc52dad23c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hands'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "request_outfit.run(\"gloves\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d899eb-9d40-4ac9-95f5-f74d3ff5a5a2",
   "metadata": {},
   "source": [
    "**NOTE!** You may have already noticed that the model can perfectly distinguish between the plural and singular forms of noun and provide an answer accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e212c18-1517-46c2-b0ae-d0f500b9ad43",
   "metadata": {},
   "source": [
    "**BE CAREFUL!** It is always possible that your model cannot provide the right answer if it is not very well trained to catch the corresponding concept proposed in the prompt. Therefore, be aware of the main objective for which a particular model has been trained and designed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd52aafa-fbf4-4203-94f1-198c638fc362",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
