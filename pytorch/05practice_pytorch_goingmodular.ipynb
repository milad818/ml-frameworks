{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6cJc5dODj0xT",
        "outputId": "8a1ec72c-8efa-4281-96c4-06ae83d9fc53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data/pizza_steak_sushi directory exists!\n",
            "Downloading...\n",
            "Unzipping...\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import requests\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "data_path = Path(\"data/\")\n",
        "image_path = data_path / \"pizza_steak_sushi\"\n",
        "\n",
        "train_dir = image_path / \"train\"\n",
        "test_dir = image_path / \"test\"\n",
        "\n",
        "if image_path.is_dir():\n",
        "  print(f\"{image_path} directory exists!\")\n",
        "else:\n",
        "  image_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "with open(data_path / \"pizza_steak_sushi.zip\", \"wb\") as f:\n",
        "  request = requests.get(\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\")\n",
        "  print(\"Downloading...\")\n",
        "  f.write(request.content)\n",
        "\n",
        "# let's unzip\n",
        "with zipfile.ZipFile(data_path / \"pizza_steak_sushi.zip\", \"r\") as zip_ref:\n",
        "  print(\"Unzipping...\")\n",
        "  zip_ref.extractall(image_path)\n",
        "\n",
        "os.remove(data_path / \"pizza_steak_sushi.zip\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x9HcXKVlO0lc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile going_modular/data_setup.py\n",
        "\"\"\"\n",
        "Contains functionality for creating PyTorch DataLoaders for\n",
        "image classification data.\n",
        "\"\"\"\n",
        "import os\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "NUM_WORKERS = os.cpu_count()\n",
        "\n",
        "def create_dataloaders(\n",
        "    train_dir: str,\n",
        "    test_dir: str,\n",
        "    transform: transforms.Compose,\n",
        "    batch_size: int,\n",
        "    num_workers: int=NUM_WORKERS\n",
        "):\n",
        "  \"\"\"Creates training and testing DataLoaders.\n",
        "\n",
        "  Takes in a training directory and testing directory path and turns\n",
        "  them into PyTorch Datasets and then into PyTorch DataLoaders.\n",
        "\n",
        "  Args:\n",
        "    train_dir: Path to training directory.\n",
        "    test_dir: Path to testing directory.\n",
        "    transform: torchvision transforms to perform on training and testing data.\n",
        "    batch_size: Number of samples per batch in each of the DataLoaders.\n",
        "    num_workers: An integer for number of workers per DataLoader.\n",
        "\n",
        "  Returns:\n",
        "    A tuple of (train_dataloader, test_dataloader, class_names).\n",
        "    Where class_names is a list of the target classes.\n",
        "    Example usage:\n",
        "      train_dataloader, test_dataloader, class_names = \\\n",
        "        = create_dataloaders(train_dir=path/to/train_dir,\n",
        "                             test_dir=path/to/test_dir,\n",
        "                             transform=some_transform,\n",
        "                             batch_size=32,\n",
        "                             num_workers=4)\n",
        "  \"\"\"\n",
        "  # Use ImageFolder to create dataset(s)\n",
        "  train_data = datasets.ImageFolder(train_dir, transform=transform)\n",
        "  test_data = datasets.ImageFolder(test_dir, transform=transform)\n",
        "\n",
        "  # Get class names\n",
        "  class_names = train_data.classes\n",
        "\n",
        "  # Turn images into data loaders\n",
        "  train_dataloader = DataLoader(\n",
        "      train_data,\n",
        "      batch_size=batch_size,\n",
        "      shuffle=True,\n",
        "      num_workers=num_workers,\n",
        "      pin_memory=True,\n",
        "  )\n",
        "  test_dataloader = DataLoader(\n",
        "      test_data,\n",
        "      batch_size=batch_size,\n",
        "      shuffle=False, # don't need to shuffle test data\n",
        "      num_workers=num_workers,\n",
        "      pin_memory=True,\n",
        "  )\n",
        "\n",
        "  return train_dataloader, test_dataloader, class_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zt5K7DLPAlko",
        "outputId": "ac43b75f-1b16-4176-ac79-cc562318a701"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing going_modular/data_setup.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import data_setup.py\n",
        "from going_modular import data_setup\n",
        "\n",
        "# Create train/test dataloader and get class names as a list\n",
        "train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(...)"
      ],
      "metadata": {
        "id": "Jjn5LJSVKy6H"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}